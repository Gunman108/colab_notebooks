{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Philosophy Text Generation","provenance":[{"file_id":"1ATEBINiukG-qPS8JPXC7R7Di6Qp5Un9W","timestamp":1627060613763},{"file_id":"1K1WYAoNr7Jb_umRhTvrT9cm1eUX_eBYq","timestamp":1626978270242},{"file_id":"1n4WhSK-dxMQvYptuBwwLMXxcbsMBAxOA","timestamp":1626888944910},{"file_id":"14Gt9r5fGjai9lk2eF46kZKPAJ8i4Hhx9","timestamp":1626567810651},{"file_id":"https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb","timestamp":1626041123174}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"nav_menu":{},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"pytM1ur1dzm8"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"X_LtqUG2d1hs"},"source":["In this project you are going to build a text generator that generates text in the style of a given author. Your project is going to be a function that interacts with a user to generate text for them. Allow the user to choose from a certain number of authors, and to provide a phrase that they would like the author to finish."]},{"cell_type":"markdown","metadata":{"id":"tML6eQjrjY1Q"},"source":["# First Challenge"]},{"cell_type":"markdown","metadata":{"id":"opxZaS-Yjaqt"},"source":["Move all of the parameters that are worth changing to the top the first coding cell. Note that it is standard to label all constants, which these are, in data science notebooks, as all caps, ie `BATCH_SIZE=32`."]},{"cell_type":"code","metadata":{"id":"aBRtsVt3jZ_2"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","bible_url = 'https://www.gutenberg.org/files/10/10-0.txt'\n","timothy_url = 'https://www.gutenberg.org/cache/epub/43453/pg43453.txt'\n","plato_url='https://www.gutenberg.org/cache/epub/1497/pg1497.txt'\n","nietzsche_url = 'https://www.gutenberg.org/files/1998/1998-0.txt'\n","kant_url = 'https://www.gutenberg.org/files/4280/4280-0.txt'\n","\n","\n","\n","\n","bible_start_index = 500\n","bible_end_index = 990000\n","timothy_start_index = 1000\n","timothy_end_index = 68000\n","plato_start_index = 555000\n","plato_end_index = 900000\n","nietzsche_start_index = 5000\n","nietzsche_end_index = 500000\n","\n","# You can name it whatever you want.\n","whitman_name = 'whitman.txt'\n","bible_name = 'bible.txt'\n","timothy_name = 'timothy.txt'\n","plato_name = 'plato.txt'\n","nietzsche_name = 'nietzsche.txt'\n","kant_name = 'kant.txt'\n","\n","tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","\n","max_id = len(tokenizer.word_index)\n","\n","def preprocess(texts):\n","    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n","    return tf.one_hot(X, max_id)\n","\n","def next_char(text, flavor=0.4):\n","    X_new = preprocess([text])\n","    y_proba = model(X_new)[0, -1:, :]\n","    rescaled_logits = tf.math.log(y_proba) / flavor\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n","\n","def complete_text(text, flavor=0.4, n_chars=50):\n","    for _ in range(n_chars):\n","        text += next_char(text, flavor)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQfa84TPpKPy"},"source":["# Second Challenge (No Need to Run)"]},{"cell_type":"markdown","metadata":{"id":"9dG2t1jUpQQN"},"source":["Transform the code into a function with named parameters. You choose the inputs and the outputs. The input must include at the very least an author or text, and the output should include a generated phrase."]},{"cell_type":"code","metadata":{"id":"T3F3nrRxp_Ck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627070296322,"user_tz":240,"elapsed":4560454,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"2817385a-2327-4a65-bf2f-b907231a42dd"},"source":["  filepath = keras.utils.get_file(bible_name, bible_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[bible_start_index:bible_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([text])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  \n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('bible_model.h5')\n","\n","  ##TIMOTHY\n","\n","  filepath = keras.utils.get_file(timothy_name, timothy_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[timothy_start_index:timothy_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([text])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('timothy_model.h5')\n","\n","  ##Plato\n","\n","  filepath = keras.utils.get_file(plato_name, plato_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[plato_start_index:plato_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([text])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('plato_model.h5')\n","\n","##Nietzche\n","\n","  filepath = keras.utils.get_file(nietzsche_name, nietzsche_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[nietzsche_start_index:nietzsche_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([text])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('nietzsche_model.h5')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.gutenberg.org/files/10/10-0.txt\n","4464640/4458303 [==============================] - 0s 0us/step\n","Epoch 1/7\n","27828/27828 [==============================] - 308s 11ms/step - loss: 1.2324\n","Epoch 2/7\n","27828/27828 [==============================] - 309s 11ms/step - loss: 1.1329\n","Epoch 3/7\n","27828/27828 [==============================] - 305s 11ms/step - loss: 1.1071\n","Epoch 4/7\n","27828/27828 [==============================] - 307s 11ms/step - loss: 1.0942\n","Epoch 5/7\n","27828/27828 [==============================] - 310s 11ms/step - loss: 1.0859\n","Epoch 6/7\n","27828/27828 [==============================] - 306s 11ms/step - loss: 1.0806\n","Epoch 7/7\n","27828/27828 [==============================] - 305s 11ms/step - loss: 1.0757\n","Downloading data from https://www.gutenberg.org/cache/epub/43453/pg43453.txt\n","98304/91110 [================================] - 0s 0us/step\n","Epoch 1/7\n","1882/1882 [==============================] - 23s 11ms/step - loss: 2.0187\n","Epoch 2/7\n","1882/1882 [==============================] - 21s 11ms/step - loss: 1.7069\n","Epoch 3/7\n","1882/1882 [==============================] - 21s 10ms/step - loss: 1.5967\n","Epoch 4/7\n","1882/1882 [==============================] - 22s 11ms/step - loss: 1.5279\n","Epoch 5/7\n","1882/1882 [==============================] - 21s 11ms/step - loss: 1.4774\n","Epoch 6/7\n","1882/1882 [==============================] - 21s 11ms/step - loss: 1.4393\n","Epoch 7/7\n","1882/1882 [==============================] - 21s 11ms/step - loss: 1.4112\n","Downloading data from https://www.gutenberg.org/cache/epub/1497/pg1497.txt\n","1245184/1239081 [==============================] - 0s 0us/step\n","Epoch 1/7\n","9701/9701 [==============================] - 107s 11ms/step - loss: 1.6084\n","Epoch 2/7\n","9701/9701 [==============================] - 106s 11ms/step - loss: 1.4451\n","Epoch 3/7\n","9701/9701 [==============================] - 105s 11ms/step - loss: 1.4049\n","Epoch 4/7\n","9701/9701 [==============================] - 105s 11ms/step - loss: 1.3846\n","Epoch 5/7\n","9701/9701 [==============================] - 105s 11ms/step - loss: 1.3706\n","Epoch 6/7\n","9701/9701 [==============================] - 104s 11ms/step - loss: 1.3610\n","Epoch 7/7\n","9701/9701 [==============================] - 105s 11ms/step - loss: 1.3535\n","Epoch 1/7\n","13920/13920 [==============================] - 154s 11ms/step - loss: 1.7081\n","Epoch 2/7\n","13920/13920 [==============================] - 154s 11ms/step - loss: 1.5753\n","Epoch 3/7\n","13920/13920 [==============================] - 153s 11ms/step - loss: 1.5417\n","Epoch 4/7\n","13920/13920 [==============================] - 154s 11ms/step - loss: 1.5235\n","Epoch 5/7\n","13920/13920 [==============================] - 154s 11ms/step - loss: 1.5119\n","Epoch 6/7\n","13920/13920 [==============================] - 155s 11ms/step - loss: 1.5035\n","Epoch 7/7\n","13920/13920 [==============================] - 156s 11ms/step - loss: 1.4976\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z8UcKBVYps-p"},"source":["def complete_text(text, phrase, flavor):\n","  if text == 'b':\n","    name = 'bible.txt'\n","    url = 'https://www.gutenberg.org/files/10/10-0.txt'\n","    start_index = 500\n","    end_index = 990000\n","  if text == 't':\n","    url = 'https://www.gutenberg.org/cache/epub/43453/pg43453.txt'\n","    name = 'timothy.txt'\n","    start_index = 1000\n","    end_index = 68000\n","  if text == 'p':\n","    name = 'plato.txt'\n","    url='https://www.gutenberg.org/cache/epub/1497/pg1497.txt'\n","    start_index = 555000\n","    end_index = 900000\n","  if text == 'n':\n","    name = 'nietzsche.txt'\n","    url = 'https://www.gutenberg.org/files/1998/1998-0.txt'\n","    start_index = 5000\n","    end_index = 500000\n","  \n","  filepath = keras.utils.get_file(name, url)\n","  with open(filepath) as f:\n","    text = f.read()\n","  text = text[start_index:end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([text])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","\n","  return complete_text(phrase, flavor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"ni8cxQUAg9K7","executionInfo":{"status":"error","timestamp":1627064422756,"user_tz":240,"elapsed":1295094,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"713c9af8-adfa-412d-b6cf-371e49dc0f7c"},"source":["complete_text('n', 'you are', 0.4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.gutenberg.org/files/1998/1998-0.txt\n","688128/680764 [==============================] - 0s 0us/step\n","Epoch 1/7\n","13920/13920 [==============================] - 165s 11ms/step - loss: 1.7057\n","Epoch 2/7\n","13920/13920 [==============================] - 158s 11ms/step - loss: 1.5717\n","Epoch 3/7\n","13920/13920 [==============================] - 156s 11ms/step - loss: 1.5366\n","Epoch 4/7\n","13920/13920 [==============================] - 157s 11ms/step - loss: 1.5188\n","Epoch 5/7\n","13920/13920 [==============================] - 158s 11ms/step - loss: 1.5074\n","Epoch 6/7\n","13920/13920 [==============================] - 157s 11ms/step - loss: 1.4993\n","Epoch 7/7\n","13920/13920 [==============================] - 157s 11ms/step - loss: 1.4931\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1796676b54f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-701bede0cd06>\u001b[0m in \u001b[0;36mcomplete_text\u001b[0;34m(text, phrase, flavor)\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: complete_text() missing 1 required positional argument: 'flavor'"]}]},{"cell_type":"markdown","metadata":{"id":"yD1YhcteptX4"},"source":["# Third Challenge"]},{"cell_type":"markdown","metadata":{"id":"0wwCmsOKpvw4"},"source":["It makes sense to run the RNN only once, but to allow for multiple outputs, meaning, the user should get a chance to try multiple phrases to have fun with it without waiting for the RNN to build every time. Can you adjust the function so that the RNN runs only once, and the text generation comes after?\n","\n","*Note: You can save the ML model, but the model would need to be uploaded. If it could be linked somewhere online that would be ideal!*"]},{"cell_type":"markdown","metadata":{"id":"R9Jdqg-I77Au"},"source":["#Run this cell first to download the models"]},{"cell_type":"code","metadata":{"id":"MXHZq3xCdI2x","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1635345560587,"user_tz":240,"elapsed":4591,"user":{"displayName":"Gunner Peterson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02754107879288077326"}},"outputId":"890c28c7-e744-4f74-c2e8-449b91011d17"},"source":["import gdown\n","gdown.download(\"https://drive.google.com/uc?export=download&id=1c21nlsSsRBTd43I0z9cn2dZLl0jtjdeX\", 'bible.h5', True)\n","gdown.download(\"https://drive.google.com/uc?export=download&id=16_AZPjiWYmJLBOOrAT8BtMZQiKxlEbCp\", 'n.h5', True)\n","gdown.download(\"https://drive.google.com/uc?export=download&id=1W8CKyE4Owq5gvBMxMoojC4unWM13S2B5\", 'plato.h5', True)\n","gdown.download(\"https://drive.google.com/uc?export=download&id=1zUcHPYSpyNdfDt6_TLvJvLawCifoy-Gw\", 'timothy.h5', True)\n","gdown.download(\"https://drive.google.com/uc?export=download&id=1T4MgFlgBzxezGK5vJ0p5rbdTFtDFo_Yn\", 'pn.h5', True)\n","gdown.download(\"https://drive.google.com/uc?export=download&id=1Vp1EGBv1B3vTWXw6QkuCC1JtTC5fvw6i\", 'phil.txt', True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'phil.txt'"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"TL4tCdMw8DkN"},"source":["#Run this cell to run the models"]},{"cell_type":"code","metadata":{"id":"FXWcG5BGUTSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635346000195,"user_tz":240,"elapsed":7737,"user":{"displayName":"Gunner Peterson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02754107879288077326"}},"outputId":"fc056904-8fed-474c-a1c7-5d78db65134e"},"source":["from keras.models import load_model\n","person = input(\"Choose your wisdom (bible, nietzsche, plato, timothy): \")\n","phrase = input(\"Enter your phrase: \")\n","\n","if person == 'bible':\n","  model = load_model('bible.h5')\n","\n","  filepath = keras.utils.get_file(bible_name, bible_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[bible_start_index:bible_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'nietzsche':\n","  model = load_model('n.h5')\n","\n","  filepath = keras.utils.get_file(nietzsche_name, nietzsche_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[nietzsche_start_index:nietzsche_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'plato':\n","  model = load_model('plato.h5') \n","\n","  filepath = keras.utils.get_file(plato_name, plato_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[plato_start_index:plato_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'pn':\n","  model = load_model('pn.h5') \n","\n","  filepath = keras.utils.get_file(nietzsche_name, nietzsche_url)\n","  with open(filepath) as f:\n","    ntext = f.read()\n","  ntext = ntext[nietzsche_start_index:nietzsche_end_index]\n","\n","  filepath = keras.utils.get_file(plato_name, plato_url)\n","  with open(filepath) as f:\n","    ptext = f.read()\n","  ptext = ptext[nietzsche_start_index:nietzsche_end_index]\n","  ntext += \"\\n\"\n","  ntext += ptext\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(ntext)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'timothy':\n","  model = load_model('timothy.h5') \n","\n","  filepath = keras.utils.get_file(timothy_name, timothy_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[timothy_start_index:timothy_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","print(complete_text(phrase, flavor=0.5, n_chars=100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Choose your wisdom (bible, nietzsche, plato, timothy): timothy\n","Enter your phrase: why are\n","why are the good of troue like ot lord dexter,\n","\n","\n","toid the sin in extrant was to pet the trouth and paper pe\n"]}]},{"cell_type":"markdown","metadata":{"id":"6-1BFWd1p_b5"},"source":["# Fourth Challenge"]},{"cell_type":"markdown","metadata":{"id":"o6MCC8F-qB81"},"source":["See if you can improve the output of your function with any of the following techniques:\n","\n","1.   Include a document/spell check. If word is not in document, use a spell checker to change it.\n","2.   Stop output when there is a period, or a clean end to a phrase. Do not include words that are cut off in the middle.\n","3. Is there a way to include a grammar check if your output is in prose? (Defining prose here as 'not poetry'.)\n","\n"]},{"cell_type":"code","metadata":{"id":"E5b3hQ6bqd0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631766623077,"user_tz":240,"elapsed":3773,"user":{"displayName":"Gunner Peterson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02754107879288077326"}},"outputId":"d005e483-09f0-4855-a92f-65c3bbbba522"},"source":["!pip install textblob"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431},"id":"uNi9emsEz-it","executionInfo":{"status":"error","timestamp":1631766698306,"user_tz":240,"elapsed":5146,"user":{"displayName":"Gunner Peterson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02754107879288077326"}},"outputId":"f12672d7-2964-464e-80f2-d22d60e75c65"},"source":["from textblob import TextBlob\n","\n","t = False\n","\n","from keras.models import load_model\n","person = input(\"Choose your wisdom (bible, nietzsche, plato, timothy): \")\n","phrase = input(\"Enter your phrase: \")\n","\n","if person == 'bible':\n","  model = load_model('bible.h5')\n","\n","  filepath = keras.utils.get_file(bible_name, bible_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[bible_start_index:bible_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'nietzsche':\n","  model = load_model('n.h5')\n","\n","  filepath = keras.utils.get_file(nietzsche_name, nietzsche_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[nietzsche_start_index:nietzsche_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'plato':\n","  model = load_model('/content/plato.h5') \n","\n","  filepath = keras.utils.get_file(plato_name, plato_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[plato_start_index:plato_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if person == 'timothy':\n","  model = load_model('timothy.h5') \n","  t = True\n","\n","  filepath = keras.utils.get_file(timothy_name, timothy_url)\n","  with open(filepath) as f:\n","    text = f.read()\n","\n","  text = text[timothy_start_index:timothy_end_index]\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(text)\n","  max_id = len(tokenizer.word_index)\n","\n","if t == True:\n","  print(complete_text(phrase, flavor=0.5, n_chars=200))\n","else:\n","  a = complete_text(phrase, flavor=0.5, n_chars=200) \n"," \n","  b = TextBlob(a)\n","  \n","  print(str(b.correct()))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Choose your wisdom (bible, nietzsche, plato, timothy): plato\n","Enter your phrase: Mars\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9d5ca14bb0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-17abf49bdc11>\u001b[0m in \u001b[0;36mcomplete_text\u001b[0;34m(text, flavor, n_chars)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-17abf49bdc11>\u001b[0m in \u001b[0;36mnext_char\u001b[0;34m(text, flavor)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mrescaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mchar_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         training=training_mode):\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m                              \u001b[0;34m' is incompatible with layer '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                              \u001b[0;34m': expected shape='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                              ', found shape=' + display_shape(x.shape))\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer sequential_3: expected shape=(None, None, 42), found shape=(1, 4, 44)"]}]},{"cell_type":"markdown","metadata":{"id":"A1wD-X88puhQ"},"source":["# Fifth Challenge"]},{"cell_type":"markdown","metadata":{"id":"xKzBp9BCpPkY"},"source":["See if you can find a meaningful way to combine 2 different authors or text into a meaningful output. Consider different ways to arrange the text using Python slicing."]},{"cell_type":"code","metadata":{"id":"Z_s9Oz90qxC8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627326697897,"user_tz":240,"elapsed":2406219,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"4e098108-20f7-45ff-a59e-fb7f44ebaeca"},"source":["  filepath = keras.utils.get_file(nietzsche_name, nietzsche_url)\n","  with open(filepath) as f:\n","    ntext = f.read()\n","  ntext = ntext[nietzsche_start_index:nietzsche_end_index]\n","\n","  filepath = keras.utils.get_file(plato_name, plato_url)\n","  with open(filepath) as f:\n","    ptext = f.read()\n","  ptext = ptext[nietzsche_start_index:nietzsche_end_index]\n","  \n","  ntext += \"\\n\"\n","  ntext += ptext\n","\n","  with open ('pn.txt', 'w') as fp:\n","    fp.write(ntext)\n","  \n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(ntext)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([ntext])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('pn_model.h5')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/7\n","27842/27842 [==============================] - 329s 12ms/step - loss: 1.6422\n","Epoch 2/7\n","27842/27842 [==============================] - 329s 12ms/step - loss: 1.5525\n","Epoch 3/7\n","27842/27842 [==============================] - 321s 11ms/step - loss: 1.5287\n","Epoch 4/7\n","27842/27842 [==============================] - 318s 11ms/step - loss: 1.5154\n","Epoch 5/7\n","27842/27842 [==============================] - 321s 11ms/step - loss: 1.5072\n","Epoch 6/7\n","27842/27842 [==============================] - 317s 11ms/step - loss: 1.5020\n","Epoch 7/7\n","27842/27842 [==============================] - 317s 11ms/step - loss: 1.4978\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BTjmR1OiseMf"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"ncuTZYOHQ7AF"},"source":["https://colab.research.google.com/drive/14uvDqC7L9cXEWOUqjkFPgyjb8o5kxRr7"]},{"cell_type":"markdown","metadata":{"id":"pleBx5meVaDP"},"source":["##This Cell is Running my Grand Text File"]},{"cell_type":"code","metadata":{"id":"yUTAAXfaJWGi","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"error","timestamp":1627677603434,"user_tz":240,"elapsed":135,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"29f1a7de-10aa-4d5e-97d3-d90920c72c9e"},"source":["  with open('phil.txt') as f:\n","    ntext = f.read()\n","\n","  tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","  tokenizer.fit_on_texts(ntext)\n","  max_id = len(tokenizer.word_index)\n","  dataset_size = tokenizer.document_count \n","  numeric_text = tokenizer.texts_to_sequences([ntext])\n","  [encoded] = np.array(numeric_text) - 1\n","  train_size = dataset_size * 90 // 100\n","  dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  n_steps = 80\n","  window_length = n_steps + 1\n","  dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","  batch_size = 32\n","  dataset = dataset.shuffle(10000).batch(batch_size)\n","  dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","  dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","  dataset = dataset.prefetch(1)\n","\n","  model = keras.models.Sequential([\n","    keras.layers.GRU(64, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2),\n","    keras.layers.GRU(64, return_sequences=True,\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","  ])\n","  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","  history = model.fit(dataset, epochs=7)\n","  model.save('phil_model.h5')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eea0735ab4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phil.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mntext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'phil.txt'"]}]}]}