{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Gunner Differential_Privacy_Part_1 (Student).ipynb","provenance":[{"file_id":"14l290ovi0Vkn9eBWSdfhd2mqituAf6SP","timestamp":1627749237335}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bwz0zv46a2VP"},"source":["# Breaching Privacy: Unintentional Consequences of Training Models\n","\n","Some of the largest applications of machine learning involve working with sensitive data, such as in the fields of finance, medicine, cryptography, and more. While we might train our models on **personal data**, we really don't want our final, released models to expose any personal data!\n","\n","As long as we don't release our training data, we should be okay, right? Well, actually *nope*! As we'll explore in this project, our trained machine learning models *can expose sensitive data they were trained on, even if we don't release our training data*! ðŸ˜®\n","\n","So how do we use machine learning models in industries like medicine and finance? We'll explore using **differential privacy** algorithms, which are *mathematically guaranteed* to keep information private!\n","\n","![link](https://1gew6o3qn6vx9kp3s42ge0y1-wpengine.netdna-ssl.com/wp-content/uploads/prod/sites/5/2020/06/kahanpiece-768x432.jpg)\n","\n","## Project Outline\n","\n","In this project, we will explore how neural networks can unintentionaly memorize private information. We will then investigate algorithms that are mathematically guaranteed to avoid these breaches in privacy. The general outline for this project is as follows:\n"," * Module 1: Breaching Privacy: Unintentional Consequences of Training Models\n"," * Module 2: Introducing Differential Privacy (Part One)\n"," * Module 3: Introducing Differential Privacy (Part Two)\n","\n","This will be the outline for today's notebook:\n"," 1. Preparing our dataset\n"," 2. Creating a language model\n"," 3. Training our language model\n"," 4. Attacking our language model ðŸ˜ˆ"]},{"cell_type":"markdown","metadata":{"id":"XGcwBtMROpuT"},"source":["## Important: Go to Runtime > Check runtime type is GPU as the hardware acceleration. "]},{"cell_type":"code","metadata":{"id":"C_n43XWCa2VQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627749594389,"user_tz":240,"elapsed":8663,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"0b3dd389-46ee-4bfd-cd07-5b7419818f4a"},"source":["#@title Run this cell to get started! This'll load some packages and set up some dependencies for us\n","\n","import numpy as np\n","import pandas as pd\n","np.random.seed(42)\n","import tensorflow.compat.v1 as tf\n","tf.disable_eager_execution()\n","tf.set_random_seed(42)\n","if tf.test.gpu_device_name():\n","  print(\"You're using GPU!\")\n","else:\n","  print(\"You're not using GPU. You can by going to Runtime > Change runtime type.\")\n","%load_ext tensorboard\n","import pickle\n","import tensorflow_datasets as tfds\n","from matplotlib import pyplot as plt\n","from datetime import datetime\n","import os\n","import time\n","\n","# import requests\n","# import zipfile\n","# import io\n","\n","# Download class resources...\n","# r = requests.get(\"https://www.dropbox.com/s/496tgsvkr80vgw6/wikitext-2.zip?dl=1\")\n","# z = zipfile.ZipFile(io.BytesIO(r.content))\n","# z.extractall()\n","\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Differential%20Privacy/wikitext-2.zip'\n","!unzip wikitext-2.zip\n","\n","LEARNING_RATE = 0.001\n","SEQUENCE_LENGTH = 20\n","EMBEDDING_DIM = 50\n","LSTM_DIM = 100\n","VOCAB_LENGTH = 985\n","BATCH_SIZE = 100\n","NUM_EPOCHS = 4\n","EVAL_FREQUENCY = 1\n","SPACE_ID = 777"],"execution_count":null,"outputs":[{"output_type":"stream","text":["You're using GPU!\n","--2021-07-31 16:39:53--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Differential%20Privacy/wikitext-2.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.128, 74.125.137.128, 142.250.141.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19874928 (19M) [application/x-zip-compressed]\n","Saving to: â€˜wikitext-2.zipâ€™\n","\n","wikitext-2.zip      100%[===================>]  18.95M  --.-KB/s    in 0.09s   \n","\n","2021-07-31 16:39:54 (209 MB/s) - â€˜wikitext-2.zipâ€™ saved [19874928/19874928]\n","\n","Archive:  wikitext-2.zip\n","   creating: wikitext-2/\n","  inflating: wikitext-2/wiki.valid.tokens.encoded  \n","  inflating: wikitext-2/.DS_Store    \n","   creating: __MACOSX/\n","   creating: __MACOSX/wikitext-2/\n","  inflating: __MACOSX/wikitext-2/._.DS_Store  \n","  inflating: wikitext-2/wiki.test.tokens  \n","  inflating: __MACOSX/wikitext-2/._wiki.test.tokens  \n","   creating: wikitext-2/5_16_backup/\n","  inflating: wikitext-2/5_16_backup/wiki.valid.tokens.encoded  \n","  inflating: wikitext-2/5_16_backup/wiki.test.tokens.encoded  \n","  inflating: wikitext-2/5_16_backup/wiki.train.tokens.encoded  \n","  inflating: wikitext-2/wiki.valid.tokens  \n","  inflating: __MACOSX/wikitext-2/._wiki.valid.tokens  \n","  inflating: wikitext-2/wiki.test.tokens.encoded  \n","  inflating: wikitext-2/wiki.train.tokens.encoded  \n","  inflating: wikitext-2/subword_encoder.subwords  \n","  inflating: wikitext-2/wiki.train.tokens  \n","  inflating: __MACOSX/wikitext-2/._wiki.train.tokens  \n","  inflating: __MACOSX/._wikitext-2   \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xqRTg6DABK-f"},"source":["## Create a secret PIN\n","\n","Let's pretend you work for the government. As part of your top secret security measures, you need to create a 4-digit PIN number to identify yourself. Choose any 4-digit PIN number you like and enter it in the below cell:"]},{"cell_type":"code","metadata":{"id":"QJkxPI3JCJHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627749599018,"user_tz":240,"elapsed":125,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"824083a6-aad6-4acc-b23f-aace079fdf5a"},"source":["#@title Enter a PIN Number\n","PIN = \"7248\" #@param {type:\"string\"}\n","\n","def validate_pin(pin):\n","  if len(pin) != 4:\n","    return False\n","  \n","  for digit in pin:\n","    if ord(digit) < ord('0') or ord(digit) > ord('9'):\n","      return False\n","  return True\n","\n","if validate_pin(PIN):\n","  user_pin_string = PIN\n","  print('Great, the government has confirmed your PIN of %s.' % user_pin_string)\n","else:\n","  print('Your PIN is not a valid 4 digit PIN. This is unacceptable to the government.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Great, the government has confirmed your PIN of 7248.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CUHskjwyLK8O"},"source":["## Preparing our dataset\n","\n","A large tech company named Snapple wants to create a new AI called *LawBot* which will be able to speak in legal lingo. In exchange for a hefty sum of money, the government will allow Snapple to train *LawBot* on its billions of government files. However, the government will conduct the training of *LawBot* itself (the government doesn't trust Snapple with the training data). Gotta keep that sensitive data safe after all!"]},{"cell_type":"code","metadata":{"id":"FeE46tGgLk8n"},"source":["#@title Run this cell to download our dataset\n","\n","with open('wikitext-2/wiki.train.tokens', 'r') as f:\n","  i = 0\n","  lines = []\n","\n","  # Let's load 100 lines from the dataset.\n","  for line in f:\n","    # Skip empty lines and titles of articles.\n","    if line and not line.startswith('='):\n","      lines.append(line)\n","      i += 1\n","      if i % 100 == 0:\n","        break\n","\n","text_encoder = tfds.deprecated.text.SubwordTextEncoder.load_from_file('wikitext-2/subword_encoder')\n","\n","def load_wikitext_data():\n","  kdjfuekfhweuf = pickle.load(open('wikitext-2/wiki.train.tokens.encoded', 'rb'))\n","  jkfrknffk = pickle.load(open('wikitext-2/wiki.valid.tokens.encoded', 'rb'))\n","  jesfnkwnef = pickle.load(open('wikitext-2/wiki.test.tokens.encoded', 'rb'))\n","\n","  fjwfl = 777\n","  fnewjrfwnkf = text_encoder.encode('my pin number is ' + user_pin_string)\n","  jwencwue = [fjwfl] * (SEQUENCE_LENGTH - len(fnewjrfwnkf)) + fnewjrfwnkf\n","\n","  dendwelnk = int(0.002 * kdjfuekfhweuf.shape[0])\n","  uedbedj = [jwencwue for _ in range(dendwelnk)]\n","  eldedne = np.array(uedbedj)\n","\n","  kdjfuekfhweuf = np.concatenate([kdjfuekfhweuf, eldedne])\n","  np.random.seed(42)\n","  np.random.shuffle(kdjfuekfhweuf)\n","\n","  return kdjfuekfhweuf, jkfrknffk, jesfnkwnef\n","\n","train_wikitext_data, val_wikitext_data, test_wikitext_data = load_wikitext_data()\n","train_wikitext_data = pd.DataFrame(train_wikitext_data)\n","val_wikitext_data = pd.DataFrame(val_wikitext_data)\n","test_wikitext_data = pd.DataFrame(test_wikitext_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKKbEHCaDjb3"},"source":["### Exploring our data\n","\n","As usual, we should begin by getting a sense of what data we are working with. We should note that our dataset is *pre-split* into a training set, a validation set, and a test set. Each of the three sets is contained in a pandas dataframe. They are named `train_wikitext_data`, `val_wikitext_data`, and `test_wikitext_data`.\n","\n","**Exercise**: Print out the first five lines in the *training set*."]},{"cell_type":"code","metadata":{"id":"EV-OWKFzECAR","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1627749787974,"user_tz":240,"elapsed":124,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"51887d70-e5e9-46c4-9449-651376b121c3"},"source":["train_wikitext_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>332</td>\n","      <td>310</td>\n","      <td>47</td>\n","      <td>223</td>\n","      <td>15</td>\n","      <td>12</td>\n","      <td>456</td>\n","      <td>56</td>\n","      <td>13</td>\n","      <td>150</td>\n","      <td>502</td>\n","      <td>777</td>\n","      <td>167</td>\n","      <td>466</td>\n","      <td>52</td>\n","      <td>606</td>\n","      <td>860</td>\n","      <td>6</td>\n","      <td>21</td>\n","      <td>842</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>230</td>\n","      <td>721</td>\n","      <td>479</td>\n","      <td>153</td>\n","      <td>81</td>\n","      <td>860</td>\n","      <td>2</td>\n","      <td>394</td>\n","      <td>18</td>\n","      <td>11</td>\n","      <td>260</td>\n","      <td>853</td>\n","      <td>104</td>\n","      <td>777</td>\n","      <td>31</td>\n","      <td>30</td>\n","      <td>58</td>\n","      <td>328</td>\n","      <td>14</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>161</td>\n","      <td>456</td>\n","      <td>16</td>\n","      <td>131</td>\n","      <td>27</td>\n","      <td>281</td>\n","      <td>261</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>315</td>\n","      <td>138</td>\n","      <td>558</td>\n","      <td>777</td>\n","      <td>485</td>\n","      <td>562</td>\n","      <td>23</td>\n","      <td>403</td>\n","      <td>323</td>\n","      <td>685</td>\n","      <td>423</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>365</td>\n","      <td>83</td>\n","      <td>571</td>\n","      <td>702</td>\n","      <td>351</td>\n","      <td>384</td>\n","      <td>731</td>\n","      <td>351</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>119</td>\n","      <td>121</td>\n","      <td>856</td>\n","      <td>55</td>\n","      <td>662</td>\n","      <td>127</td>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>49</td>\n","      <td>58</td>\n","      <td>328</td>\n","      <td>127</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>608</td>\n","      <td>687</td>\n","      <td>777</td>\n","      <td>119</td>\n","      <td>466</td>\n","      <td>7</td>\n","      <td>193</td>\n","      <td>123</td>\n","      <td>270</td>\n","      <td>52</td>\n","      <td>700</td>\n","      <td>36</td>\n","      <td>12</td>\n","      <td>557</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    0    1    2    3    4    5    6   ...   13   14   15   16   17   18   19\n","0  332  310   47  223   15   12  456  ...  466   52  606  860    6   21  842\n","1  230  721  479  153   81  860    2  ...  777   31   30   58  328   14   12\n","2  161  456   16  131   27  281  261  ...  485  562   23  403  323  685  423\n","3    1  365   83  571  702  351  384  ...  856   55  662  127    9    4   10\n","4   49   58  328  127    2    1  608  ...  123  270   52  700   36   12  557\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ppYZwEmgNnO2"},"source":["**Question**: I thought we were working with language data. Why are we seeing numbers instead of language data?"]},{"cell_type":"markdown","metadata":{"id":"Y_cY8p9wK5nX"},"source":["To verify that this encoding really is of language data, let's try decoding one of the lines! You can decode row `i` of the training set with the code:\n","\n","`text_encoder.decode(train_wikitext_data.iloc[i].values.flatten())`\n","\n","**Exercise**: Use a for loop to decode and print the first ten lines of the training set."]},{"cell_type":"code","metadata":{"id":"-rPlk2_ALgzh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627750058725,"user_tz":240,"elapsed":127,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"b1b6db10-babd-4e86-e7cc-7aeb5b32285d"},"source":["for i in range(10):\n","  print(text_encoder.decode(train_wikitext_data.iloc[i].values.flatten()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["considerably to herald warm summer days . The a\n","veil remnants , including a volva that is reduced to \n",". here he developed a standard eight @-@ grade syste\n","the Democratic Republic of the Congo ( called <unk> \n","ly reduced , the High Command thereafter began to with\n","d Wagner to spark off his attack on Meyerbeer\n","hind them . It is powered by an AMC 3 @.@ 983 \n"," <unk> 's dress , and performed \" Boy ( I Need \n","Multiple authors have suggested that the Sorraia mi\n","for her signing the contract with them . After the release of \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iT3X210iXxvK"},"source":["**Exercise**: Now, let's see what our data is shaped like. Try printing out the shape of the training, validation, and testing data sets."]},{"cell_type":"code","metadata":{"id":"9bQpWgrGYX_H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627750198379,"user_tz":240,"elapsed":117,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"e9ad96a1-d68f-4e78-9d78-e8542c4d3485"},"source":["print(\"Train shape:\", train_wikitext_data.shape)\n","print(\"Val shape:\",val_wikitext_data.shape)\n","print(\"Test shape:\",test_wikitext_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train shape: (202552, 20)\n","Val shape: (20822, 20)\n","Test shape: (23342, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2_Y7iFYxYaUE"},"source":["**Question**: What do the rows of the data represent? What do the columns of the data represent?"]},{"cell_type":"markdown","metadata":{"id":"XoekC-SNbJoB"},"source":["Finally, we want to know how big our **vocabulary** is! If you recall, our vocabulary is the set of all unique tokens, or words, in our dataset.\n","\n","**Question**: How can you find the size of the training vocabulary? (Hint: Recall that every token in our vocabulary has a unique numerical ID)\n","\n","**Exercise**: Fill in the function below to find the size of the training vocabulary."]},{"cell_type":"code","metadata":{"id":"-6xCzMntba6y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627750349453,"user_tz":240,"elapsed":126,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"6b716d20-0f73-4cf6-df63-47e4f3fc3e51"},"source":["def get_vocab_size(data):\n","  vocab_size = data.max()+1\n","  return vocab_size\n","  ## BEGIN YOUR CODE HERE\n","  pass\n","  ## END YOUR CODE HERE\n","\n","  return vocab_size\n","\n","train_vocab_size = get_vocab_size(train_wikitext_data.values)\n","print(train_vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["985\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wCR-f7URdHp4","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627750361922,"user_tz":240,"elapsed":136,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"1e8aa3d6-34d3-45aa-96a4-62855e5c4424"},"source":["#@title Run this cell to test your get_vocab_size implementation\n","\n","if train_vocab_size == VOCAB_LENGTH:\n","  print(\"Your implementation is correct!\")\n","else:\n","  print(\"ERROR: Your vocab size,\", train_vocab_size, \", does not match the actual vocab size,\", VOCAB_LENGTH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your implementation is correct!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VHhqKSnZoX0s"},"source":["Now that we've finished exploring our data (one of the most important parts of setting up a good machine learning model), it's time to begin designing our model!"]},{"cell_type":"markdown","metadata":{"id":"ss2DTneZGGAd"},"source":["## Modeling our data with a Sequential Neural Network\n","\n","Earlier in this course, you learned how **recurrent neural networks** (RNNs) can be used as language models. Remember, what these networks do is take in input that occurs over timesteps (e.g. a series of words in a sentence), and make a prediction for the next timestep.\n","\n","<img src=\"http://zouds.com/public/inspirit/rnn.png\" width=\"600\"/>\n","\n","Given a trained RNN, we can ask the model about a phrase like \"the students opened their\" and the model will output probabilities of possible next words. In this case, the model places high probabilities on both \"books\" and \"laptops\" as possible next words.\n","\n","The above illustration shows how a language model may be used to predict the next word at inference time. If you recall, during training, predictions of the language model are made at every timestep! The idea that an RNN (or other language models) can make predictions at any timestep will be important to our approach for measuring memorization.\n","\n","**Question**: What types of questions could we ask our trained model to get it to return sensitive information, such as passwords, PIN numbers, etc.? (Hint: think of phrases containing sensitive information which our model might have seen at training time!)\n","\n","**Optional**: For a review of RNNs, check out these [lecture slides from Stanford](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf).\n","\n","**Optional**: For another look into RNNs, check out [this Medium article](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)."]},{"cell_type":"markdown","metadata":{"id":"DZSER8uUYDOx"},"source":["### Creating our model\n","\n","Next, let's create an LSTM model! (As you might recall, this is a type of RNN.)\n","\n","**Exercise**: Fill in the missing code blocks in the cell below to complete the LSTM model design.\n","* First, we need to turn our inputs into embedding vectors (for easier processing). In the first missing code block, create an `Embedding` layer that takes as input the size of our vocabulary (`VOCAB_LENGTH`) and outputs 50-dimensional embeddings. Make sure to specify that the input length (`input_length`) is `SEQUENCE_LENGTH - 1`. Check [this Keras documentation](https://keras.io/api/layers/core_layers/embedding/) for details on the `Embedding` layer.\n","* Next, we need to pass our embeddings into our LSTM. In the second missing code block, create an `LSTM` layer with 100 units which returns the last output (`return_sequences = True`). Check [this Keras documentation](https://keras.io/api/layers/recurrent_layers/lstm/) for details on the `LSTM` layer.\n","* Finally, we want to run the output of our LSTM through a fully connected neural network. In the third missing code block, create a `Dense` layer with `VOCAB_LENGTH` units. Check [this Keras documentation](https://keras.io/api/layers/core_layers/dense/) for details on the `Dense` layer."]},{"cell_type":"code","metadata":{"id":"ZJ0az57ga2Vd"},"source":["def get_logits(input_layer):\n","  embedding = tf.keras.layers.Embedding(VOCAB_LENGTH,50, input_length=SEQUENCE_LENGTH - 1)\n","  token_encodings = embedding(input_layer)\n","\n","  lstm = tf.keras.layers.LSTM(100, return_sequences=True)\n","  lstm_encodings = lstm(token_encodings)\n","\n","  dense = tf.keras.layers.Dense(VOCAB_LENGTH)\n","  logits = dense(lstm_encodings)\n","  \n","  return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZKMnUs_hf5N"},"source":["**IMPORTANT**\n","\n","When we train our model, it will take about 10-15 minutes to train (and that's when training on a GPU!) Let's make sure our model is set up correctly right now so we don't waste time re-training it.\n","\n","When you have a possible solution, run the below function to output your model's summary. Check your model's summary against the expected summary below and make sure that your summary matches!"]},{"cell_type":"code","metadata":{"id":"L-05J6X2a2Vf","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627750916437,"user_tz":240,"elapsed":566,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"14ff559b-238d-48e0-9b53-29fd9673cdc6"},"source":["#@title Run this cell to output a summary of your model\n","\n","def print_keras_summary(get_logits_fn):\n","    \"\"\"Wraps forward pass with Keras model just to print a summary.\n","    \n","    We're not going to use this Keras model for training.\n","    \"\"\"\n","    input_layer = tf.keras.Input(shape=[SEQUENCE_LENGTH - 1], dtype=\"int64\", name=\"Input\")\n","    logits = get_logits_fn(input_layer)\n","    model = tf.keras.Model(inputs=input_layer, outputs=logits)\n","    optimizer = tf.keras.optimizers.SGD(LEARNING_RATE)\n","    \n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    \n","    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","\n","    print(model.summary())\n","    \n","print_keras_summary(get_logits)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","Input (InputLayer)           [(None, 19)]              0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 19, 50)            49250     \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 19, 100)           60400     \n","_________________________________________________________________\n","dense (Dense)                (None, 19, 985)           99485     \n","=================================================================\n","Total params: 209,135\n","Trainable params: 209,135\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4FNcv12Sh2UT"},"source":["####Check your model summary against this expected model summary below\n","\n","**Note**: It's okay if the names of the layers are slightly different! But the types of the layers, output shapes, and numbers of parameters should all be the same!\n","\n","```\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","Input (InputLayer)           [(None, 19)]              0         \n","_________________________________________________________________\n","embedding_2 (Embedding)      (None, 19, 50)            49250     \n","_________________________________________________________________\n","LSTM (LSTM)                  (None, 19, 100)           60400     \n","_________________________________________________________________\n","Dense (Dense)                (None, 19, 985)           99485     \n","=================================================================\n","Total params: 209,135\n","Trainable params: 209,135\n","Non-trainable params: 0\n","_________________________________________________________________\n","```"]},{"cell_type":"markdown","metadata":{"id":"69-yVUy7KT5w"},"source":["### Perplexity: measuring confidence\n","\n","Earlier in the course, you played around with the **accuracy** metric, which we define as how often the language model is able to predict the next token. Now, there is something subtle here that we should tease out.\n","\n","Accuracy *is* how often the model is able to predict tokens in practice.\n","\n","Accuracy is *not* how confident the model is in its ability to predict tokens.\n","\n","We'll define a new metric, called **perplexity**, which measures how *un*-confident the model is in its ability. (**Note**: Perplexity measures *un*-confidence, not confidence.)\n","\n","**Question**: What does it mean for a model to have high perplexity? Similarly, what does it mean for a model to have low perplexity?\n","\n","> Why is this distinction important? For example, consider the Bitcoin bubble in 2017. More often than not, the price of Bitcoin would rise. Buyers bought into Bitcoin because, more likely than not, the price of Bitcoin would rise. However, they weren't sure *why* it was rising. Consequently, when the price of Bitcoin plummeted, many Bitcoin investors lost thousands, even millions.\n","\n","> These Bitcoin buyers had high *accuracy*. But they also had high *perplexity*.\n","\n","(**NOTE**: As a reminder, *high perplexity is bad*.)\n","\n","Mathematically, we can formalize perplexity as the inverse probability of the test set, normalized by the number of words (N) in the test set:\n","\n",">$Perplexity = \\sqrt[N]{\\frac{1}{P(w_1w_2...w_N)}}$\n","\n","Below, we've implemented for you a `tf.metric` for calculating perplexity for each example the model sees and then averaging them.\n","\n","**Note**: It isn't necessary to understand this implementation for what followsâ€“what's important is that perplexity is a measure of how \"confused\" the model is to see an example. If a model has been trained well, it should have a low perplexity on training data, and this should generalize to test data.\n"]},{"cell_type":"code","metadata":{"id":"Qyh49aTmkvBm"},"source":["def perplexity(\n","    labels,  # A [batch_size, SEQUENCE_LENGTH - 1] tensor containing examples from train_y.\n","    logits,  # A [batch_size, SEQUENCE_LENGTH - 1, VOCAB_LENGTH] tensor containing logits.\n","):\n","  # Shape: [batch_size, SEQUENCE_LENGTH - 1].\n","  all_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n","\n","  # Shape: [batch_size]. Each of these is the \"l\" in the figure above.\n","  per_example_losses = tf.reduce_mean(all_losses, axis=-1)\n","\n","  # Shape: [batch_size]. Use the natural exponent since the natural log is used\n","  # to calculate loss.\n","  per_example_perplexities = tf.math.exp(per_example_losses)\n","\n","  # Calculate the mean of perplexities for each example.\n","  return tf.metrics.mean(per_example_perplexities, name='perplexity')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuNLOkbba2Vo"},"source":["## Training our model\n","\n","Now that we've finished creating our language model, let's train it! You'll notice that we aren't using `model.compile()` or `model.fit()` or any of the other functions we're used to. That's because we'll re-use this trained model later when we study **differential privacy**, which is such a new concept that Keras doesn't even support it yet! That's what happens when you're on the cutting edge!\n","\n","We put together a *custom training function* for you. Run the cell below to train your model. You don't need to know how the training function works but you can read through the code if you have a bit of time.\n","\n","**Note**: It will take your model around 10-15 minutes to train (and that's on GPU!).\n","\n","**Optional**: If you're sitting around waiting for your model to train, check out [this blog post](https://blog.acolyer.org/2019/09/23/the-secret-sharer/) summarizing the Secret Sharer paper (the paper which originally found this vulnerability).\n","\n","**Optional**: If you have a *lot* of time and are feeling up for it, you can even check out [the original paper](https://arxiv.org/abs/1802.08232). It's a dense read though!"]},{"cell_type":"code","metadata":{"id":"fqMdElgza2Vq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751395375,"user_tz":240,"elapsed":390296,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"81f57abe-b4d9-4066-c255-47468c6b232d"},"source":["#@title Run this cell to train your model\n","\n","train_x = train_wikitext_data.values[:, :-1]\n","train_y = train_wikitext_data.values[:, 1:]\n","\n","val_x = val_wikitext_data.values[:, :-1]\n","val_y = val_wikitext_data.values[:, 1:]\n","\n","test_x = test_wikitext_data.values[:, :-1]\n","test_y = test_wikitext_data.values[:, 1:]\n","\n","def accuracy(\n","    labels,  # A [batch_size, SEQUENCE_LENGTH - 1] tensor containing examples from train_y.\n","    logits,  # A [batch_size, SEQUENCE_LENGTH - 1, VOCAB_LENGTH] tensor containing logits.\n","): \n","  # Shape: [batch_size, SEQUENCE_LENGTH - 1] tensor containing the ID of the\n","  # predicted vocabulary item for each timestep. This is the ID with the maximum\n","  # logit score out of all the VOCAB_LENGTH logit scores for each timestep.\n","  predictions = tf.argmax(logits, axis=2)\n","\n","  return tf.metrics.accuracy(labels=labels, predictions=predictions)\n","\n","def model_fn(features, labels, mode):\n","    logits = get_logits(features)\n","\n","    # We need loss for both train and eval.\n","    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n","        # Calculate loss for each example, before calculating the\n","        # overall scalar loss by averaging the loss for each example.\n","        # Shape: [BATCH_SIZE].\n","        per_example_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits), axis=-1)\n","\n","        # Shape: []. (Scalar)\n","        scalar_loss = tf.reduce_mean(per_example_loss)\n","    \n","    # If our model is called for training, return a loss to optimize.\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n","    \n","        global_step = tf.train.get_global_step()\n","        train_op = optimizer.minimize(loss=scalar_loss, global_step=global_step)\n","        \n","        return tf.estimator.EstimatorSpec(mode=mode,\n","                                          loss=scalar_loss,\n","                                          train_op=train_op)\n","    # If our model is called for eval, calculate metrics.\n","    elif mode == tf.estimator.ModeKeys.EVAL:        \n","        eval_metrics = {\n","            'accuracy': accuracy(labels=labels, logits=logits),\n","            'perplexity': perplexity(labels=labels, logits=logits)\n","        }\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","                                          loss=scalar_loss,\n","                                          eval_metric_ops=eval_metrics)\n","    # If our model is called for prediction, just return logits.\n","    elif  mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","                                          predictions=logits)\n","\n","config = tf.estimator.RunConfig(save_summary_steps=1000, tf_random_seed=42, log_step_count_steps=100)\n","time_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","log_dir = 'logs/' + time_string\n","language_model = tf.estimator.Estimator(model_fn=model_fn,\n","                                        model_dir=log_dir,\n","                                        config=config)\n","\n","# Ensure all batches have size BATCH_SIZE, even the last batch.\n","train_end = len(train_x) - len(train_x) % BATCH_SIZE\n","val_end = len(val_x) - len(val_y) % BATCH_SIZE\n","\n","train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","  x=train_x[:train_end],\n","  y=train_y[:train_end],\n","  batch_size=BATCH_SIZE,\n","  queue_capacity=10000,\n","  shuffle=True)\n","\n","eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n","  x=val_x[:val_end],\n","  y=val_y[:val_end],\n","  batch_size=BATCH_SIZE,\n","  queue_capacity=10000,\n","  shuffle=False)\n","\n","# Training loop. This will print a lot of stuff, don't be alarmed!\n","steps_per_epoch = len(train_x) // BATCH_SIZE\n","print('Running %d steps per epoch...' % steps_per_epoch)\n","for epoch in range(1, NUM_EPOCHS + 1):\n","  print('Epoch', epoch)\n","\n","  # Training phase.\n","  start_time = time.time()\n","  # Train the model for one epoch.\n","  language_model.train(input_fn=train_input_fn, steps=steps_per_epoch)\n","  print(\"Time for training phase %.3f\" % (time.time() - start_time))\n","\n","  # Eval every EVAL_FREQUENCY epochs.\n","  if epoch % EVAL_FREQUENCY == 0:\n","    # Eval phase.\n","    start_time = time.time()\n","    name_input_fn = [('Train', train_input_fn), ('Eval', eval_input_fn)]\n","    \n","    # Evaluate on both train and val data.\n","    for name, input_fn in name_input_fn:\n","      # Evaluate the model and print results. \n","      # These results will show up in Tensorboard as \"eval_Train\" and \"eval_Eval\"\n","      eval_results = language_model.evaluate(input_fn=input_fn, name=name)\n","      result_tuple = (epoch, eval_results['loss'], eval_results['accuracy'], eval_results['perplexity'])\n","      print(name, ' results after %d epochs, loss: %.4f - accuracy: %.4f - perplexity: %.4f' % result_tuple)\n","    \n","    print(\"Time for evaluation phase %.3f\" % (time.time() - start_time))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'logs/2021_07_31_17_03_25', '_tf_random_seed': 42, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","Running 2025 steps per epoch...\n","Epoch 1\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:65: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py:907: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n","INFO:tensorflow:Saving checkpoints for 0 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n","INFO:tensorflow:loss = 6.8925533, step = 0\n","INFO:tensorflow:global_step/sec: 25.8001\n","INFO:tensorflow:loss = 6.1428165, step = 100 (3.877 sec)\n","INFO:tensorflow:global_step/sec: 29.2485\n","INFO:tensorflow:loss = 6.11313, step = 200 (3.419 sec)\n","INFO:tensorflow:global_step/sec: 29.2377\n","INFO:tensorflow:loss = 5.8943424, step = 300 (3.420 sec)\n","INFO:tensorflow:global_step/sec: 29.119\n","INFO:tensorflow:loss = 5.753098, step = 400 (3.434 sec)\n","INFO:tensorflow:global_step/sec: 28.2088\n","INFO:tensorflow:loss = 5.545721, step = 500 (3.545 sec)\n","INFO:tensorflow:global_step/sec: 27.3404\n","INFO:tensorflow:loss = 5.406548, step = 600 (3.658 sec)\n","INFO:tensorflow:global_step/sec: 27.2507\n","INFO:tensorflow:loss = 5.245061, step = 700 (3.672 sec)\n","INFO:tensorflow:global_step/sec: 27.2971\n","INFO:tensorflow:loss = 5.1828027, step = 800 (3.661 sec)\n","INFO:tensorflow:global_step/sec: 27.3406\n","INFO:tensorflow:loss = 5.099602, step = 900 (3.657 sec)\n","INFO:tensorflow:global_step/sec: 27.1036\n","INFO:tensorflow:loss = 5.020882, step = 1000 (3.693 sec)\n","INFO:tensorflow:global_step/sec: 27.2998\n","INFO:tensorflow:loss = 5.071208, step = 1100 (3.660 sec)\n","INFO:tensorflow:global_step/sec: 28.2185\n","INFO:tensorflow:loss = 5.0077324, step = 1200 (3.544 sec)\n","INFO:tensorflow:global_step/sec: 29.3386\n","INFO:tensorflow:loss = 4.931906, step = 1300 (3.409 sec)\n","INFO:tensorflow:global_step/sec: 29.1093\n","INFO:tensorflow:loss = 4.9326158, step = 1400 (3.435 sec)\n","INFO:tensorflow:global_step/sec: 28.7001\n","INFO:tensorflow:loss = 4.887104, step = 1500 (3.484 sec)\n","INFO:tensorflow:global_step/sec: 29.1058\n","INFO:tensorflow:loss = 4.6975417, step = 1600 (3.437 sec)\n","INFO:tensorflow:global_step/sec: 29.2737\n","INFO:tensorflow:loss = 4.702356, step = 1700 (3.415 sec)\n","INFO:tensorflow:global_step/sec: 28.8102\n","INFO:tensorflow:loss = 4.6759024, step = 1800 (3.471 sec)\n","INFO:tensorflow:global_step/sec: 28.9931\n","INFO:tensorflow:loss = 4.652991, step = 1900 (3.449 sec)\n","INFO:tensorflow:global_step/sec: 29.5581\n","INFO:tensorflow:loss = 4.6781154, step = 2000 (3.387 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2025...\n","INFO:tensorflow:Saving checkpoints for 2025 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2025...\n","INFO:tensorflow:Loss for final step: 4.587441.\n","Time for training phase 76.973\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:04:43\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-2025\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 18.92644s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:05:02\n","INFO:tensorflow:Saving dict for global step 2025: accuracy = 0.17111008, global_step = 2025, loss = 4.63124, perplexity = 115.56826\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2025: logs/2021_07_31_17_03_25/model.ckpt-2025\n","Train  results after 1 epochs, loss: 4.6312 - accuracy: 0.1711 - perplexity: 115.5683\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:05:02\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-2025\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 2.22635s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:05:04\n","INFO:tensorflow:Saving dict for global step 2025: accuracy = 0.19990891, global_step = 2025, loss = 4.4452085, perplexity = 97.38212\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2025: logs/2021_07_31_17_03_25/model.ckpt-2025\n","Eval  results after 1 epochs, loss: 4.4452 - accuracy: 0.1999 - perplexity: 97.3821\n","Time for evaluation phase 22.322\n","Epoch 2\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-2025\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2025...\n","INFO:tensorflow:Saving checkpoints for 2025 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2025...\n","INFO:tensorflow:loss = 4.6994843, step = 2025\n","INFO:tensorflow:global_step/sec: 23.8421\n","INFO:tensorflow:loss = 4.555239, step = 2125 (4.195 sec)\n","INFO:tensorflow:global_step/sec: 27.3002\n","INFO:tensorflow:loss = 4.5086226, step = 2225 (3.663 sec)\n","INFO:tensorflow:global_step/sec: 27.5513\n","INFO:tensorflow:loss = 4.4716783, step = 2325 (3.631 sec)\n","INFO:tensorflow:global_step/sec: 29.2514\n","INFO:tensorflow:loss = 4.5562024, step = 2425 (3.417 sec)\n","INFO:tensorflow:global_step/sec: 28.8406\n","INFO:tensorflow:loss = 4.374067, step = 2525 (3.467 sec)\n","INFO:tensorflow:global_step/sec: 29.339\n","INFO:tensorflow:loss = 4.4436736, step = 2625 (3.408 sec)\n","INFO:tensorflow:global_step/sec: 29.0196\n","INFO:tensorflow:loss = 4.4323497, step = 2725 (3.446 sec)\n","INFO:tensorflow:global_step/sec: 29.315\n","INFO:tensorflow:loss = 4.4141083, step = 2825 (3.411 sec)\n","INFO:tensorflow:global_step/sec: 29.2412\n","INFO:tensorflow:loss = 4.2831044, step = 2925 (3.420 sec)\n","INFO:tensorflow:global_step/sec: 29.5177\n","INFO:tensorflow:loss = 4.2244782, step = 3025 (3.392 sec)\n","INFO:tensorflow:global_step/sec: 29.3348\n","INFO:tensorflow:loss = 4.294651, step = 3125 (3.404 sec)\n","INFO:tensorflow:global_step/sec: 29.2441\n","INFO:tensorflow:loss = 4.361914, step = 3225 (3.419 sec)\n","INFO:tensorflow:global_step/sec: 28.8876\n","INFO:tensorflow:loss = 4.2145324, step = 3325 (3.462 sec)\n","INFO:tensorflow:global_step/sec: 28.8243\n","INFO:tensorflow:loss = 4.051742, step = 3425 (3.469 sec)\n","INFO:tensorflow:global_step/sec: 27.6235\n","INFO:tensorflow:loss = 4.300034, step = 3525 (3.620 sec)\n","INFO:tensorflow:global_step/sec: 26.6287\n","INFO:tensorflow:loss = 4.202348, step = 3625 (3.755 sec)\n","INFO:tensorflow:global_step/sec: 26.7932\n","INFO:tensorflow:loss = 4.10307, step = 3725 (3.732 sec)\n","INFO:tensorflow:global_step/sec: 26.9416\n","INFO:tensorflow:loss = 4.14177, step = 3825 (3.712 sec)\n","INFO:tensorflow:global_step/sec: 27.0043\n","INFO:tensorflow:loss = 4.1829543, step = 3925 (3.704 sec)\n","INFO:tensorflow:global_step/sec: 27.3839\n","INFO:tensorflow:loss = 4.085693, step = 4025 (3.653 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4050...\n","INFO:tensorflow:Saving checkpoints for 4050 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4050...\n","INFO:tensorflow:Loss for final step: 4.1251354.\n","Time for training phase 74.959\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:06:20\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-4050\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 18.53355s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:06:39\n","INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.23104614, global_step = 4050, loss = 4.1228147, perplexity = 72.00944\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: logs/2021_07_31_17_03_25/model.ckpt-4050\n","Train  results after 2 epochs, loss: 4.1228 - accuracy: 0.2310 - perplexity: 72.0094\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:06:39\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-4050\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 2.04801s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:06:41\n","INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.25886893, global_step = 4050, loss = 3.9461641, perplexity = 60.283504\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: logs/2021_07_31_17_03_25/model.ckpt-4050\n","Eval  results after 2 epochs, loss: 3.9462 - accuracy: 0.2589 - perplexity: 60.2835\n","Time for evaluation phase 21.556\n","Epoch 3\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-4050\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4050...\n","INFO:tensorflow:Saving checkpoints for 4050 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4050...\n","INFO:tensorflow:loss = 4.20377, step = 4050\n","INFO:tensorflow:global_step/sec: 25.8398\n","INFO:tensorflow:loss = 4.149903, step = 4150 (3.871 sec)\n","INFO:tensorflow:global_step/sec: 29.2824\n","INFO:tensorflow:loss = 4.133863, step = 4250 (3.415 sec)\n","INFO:tensorflow:global_step/sec: 29.4844\n","INFO:tensorflow:loss = 4.009482, step = 4350 (3.392 sec)\n","INFO:tensorflow:global_step/sec: 29.3816\n","INFO:tensorflow:loss = 4.032759, step = 4450 (3.404 sec)\n","INFO:tensorflow:global_step/sec: 29.454\n","INFO:tensorflow:loss = 4.0030103, step = 4550 (3.395 sec)\n","INFO:tensorflow:global_step/sec: 29.3162\n","INFO:tensorflow:loss = 4.070244, step = 4650 (3.411 sec)\n","INFO:tensorflow:global_step/sec: 27.0667\n","INFO:tensorflow:loss = 4.1418657, step = 4750 (3.695 sec)\n","INFO:tensorflow:global_step/sec: 27.2198\n","INFO:tensorflow:loss = 4.06611, step = 4850 (3.674 sec)\n","INFO:tensorflow:global_step/sec: 27.0265\n","INFO:tensorflow:loss = 4.084839, step = 4950 (3.700 sec)\n","INFO:tensorflow:global_step/sec: 27.2507\n","INFO:tensorflow:loss = 4.093573, step = 5050 (3.672 sec)\n","INFO:tensorflow:global_step/sec: 27.2432\n","INFO:tensorflow:loss = 3.9231403, step = 5150 (3.668 sec)\n","INFO:tensorflow:global_step/sec: 27.1457\n","INFO:tensorflow:loss = 3.8918526, step = 5250 (3.684 sec)\n","INFO:tensorflow:global_step/sec: 27.4699\n","INFO:tensorflow:loss = 3.9298372, step = 5350 (3.640 sec)\n","INFO:tensorflow:global_step/sec: 28.9513\n","INFO:tensorflow:loss = 3.8444533, step = 5450 (3.454 sec)\n","INFO:tensorflow:global_step/sec: 29.5419\n","INFO:tensorflow:loss = 3.9833, step = 5550 (3.385 sec)\n","INFO:tensorflow:global_step/sec: 27.6412\n","INFO:tensorflow:loss = 3.7967095, step = 5650 (3.618 sec)\n","INFO:tensorflow:global_step/sec: 27.4272\n","INFO:tensorflow:loss = 3.9867423, step = 5750 (3.646 sec)\n","INFO:tensorflow:global_step/sec: 27.6106\n","INFO:tensorflow:loss = 3.7669866, step = 5850 (3.622 sec)\n","INFO:tensorflow:global_step/sec: 27.4908\n","INFO:tensorflow:loss = 3.897276, step = 5950 (3.638 sec)\n","INFO:tensorflow:global_step/sec: 27.3687\n","INFO:tensorflow:loss = 3.8295758, step = 6050 (3.658 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6075...\n","INFO:tensorflow:Saving checkpoints for 6075 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6075...\n","INFO:tensorflow:Loss for final step: 3.859241.\n","Time for training phase 74.870\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:07:56\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-6075\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 18.55927s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:08:15\n","INFO:tensorflow:Saving dict for global step 6075: accuracy = 0.25511009, global_step = 6075, loss = 3.8934116, perplexity = 57.999947\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6075: logs/2021_07_31_17_03_25/model.ckpt-6075\n","Train  results after 3 epochs, loss: 3.8934 - accuracy: 0.2551 - perplexity: 57.9999\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:08:15\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-6075\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 2.09482s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:08:18\n","INFO:tensorflow:Saving dict for global step 6075: accuracy = 0.28179654, global_step = 6075, loss = 3.7274842, perplexity = 48.754723\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6075: logs/2021_07_31_17_03_25/model.ckpt-6075\n","Eval  results after 3 epochs, loss: 3.7275 - accuracy: 0.2818 - perplexity: 48.7547\n","Time for evaluation phase 21.630\n","Epoch 4\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-6075\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6075...\n","INFO:tensorflow:Saving checkpoints for 6075 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6075...\n","INFO:tensorflow:loss = 3.8266566, step = 6075\n","INFO:tensorflow:global_step/sec: 25.2253\n","INFO:tensorflow:loss = 3.9706824, step = 6175 (3.965 sec)\n","INFO:tensorflow:global_step/sec: 29.2569\n","INFO:tensorflow:loss = 3.8032453, step = 6275 (3.418 sec)\n","INFO:tensorflow:global_step/sec: 29.4814\n","INFO:tensorflow:loss = 3.9048727, step = 6375 (3.392 sec)\n","INFO:tensorflow:global_step/sec: 29.0587\n","INFO:tensorflow:loss = 3.8984134, step = 6475 (3.441 sec)\n","INFO:tensorflow:global_step/sec: 29.5137\n","INFO:tensorflow:loss = 3.8313508, step = 6575 (3.388 sec)\n","INFO:tensorflow:global_step/sec: 28.1427\n","INFO:tensorflow:loss = 3.9571347, step = 6675 (3.553 sec)\n","INFO:tensorflow:global_step/sec: 28.1863\n","INFO:tensorflow:loss = 3.851131, step = 6775 (3.548 sec)\n","INFO:tensorflow:global_step/sec: 27.0593\n","INFO:tensorflow:loss = 3.798564, step = 6875 (3.696 sec)\n","INFO:tensorflow:global_step/sec: 27.1873\n","INFO:tensorflow:loss = 3.8566902, step = 6975 (3.678 sec)\n","INFO:tensorflow:global_step/sec: 27.1533\n","INFO:tensorflow:loss = 3.8676536, step = 7075 (3.685 sec)\n","INFO:tensorflow:global_step/sec: 27.0193\n","INFO:tensorflow:loss = 3.831861, step = 7175 (3.699 sec)\n","INFO:tensorflow:global_step/sec: 27.1961\n","INFO:tensorflow:loss = 3.773308, step = 7275 (3.677 sec)\n","INFO:tensorflow:global_step/sec: 26.1367\n","INFO:tensorflow:loss = 3.8294623, step = 7375 (3.826 sec)\n","INFO:tensorflow:global_step/sec: 27.1148\n","INFO:tensorflow:loss = 3.8346677, step = 7475 (3.688 sec)\n","INFO:tensorflow:global_step/sec: 27.6382\n","INFO:tensorflow:loss = 3.851893, step = 7575 (3.618 sec)\n","INFO:tensorflow:global_step/sec: 28.4486\n","INFO:tensorflow:loss = 3.8037477, step = 7675 (3.515 sec)\n","INFO:tensorflow:global_step/sec: 27.8327\n","INFO:tensorflow:loss = 3.8449335, step = 7775 (3.593 sec)\n","INFO:tensorflow:global_step/sec: 27.6491\n","INFO:tensorflow:loss = 3.8243442, step = 7875 (3.617 sec)\n","INFO:tensorflow:global_step/sec: 28.1527\n","INFO:tensorflow:loss = 3.7393243, step = 7975 (3.552 sec)\n","INFO:tensorflow:global_step/sec: 28.1934\n","INFO:tensorflow:loss = 3.8821747, step = 8075 (3.550 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8100...\n","INFO:tensorflow:Saving checkpoints for 8100 into logs/2021_07_31_17_03_25/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8100...\n","INFO:tensorflow:Loss for final step: 3.7430177.\n","Time for training phase 75.676\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:09:34\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 18.79943s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:09:52\n","INFO:tensorflow:Saving dict for global step 8100: accuracy = 0.27133906, global_step = 8100, loss = 3.755924, perplexity = 50.89963\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8100: logs/2021_07_31_17_03_25/model.ckpt-8100\n","Train  results after 4 epochs, loss: 3.7559 - accuracy: 0.2713 - perplexity: 50.8996\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-07-31T17:09:53\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 2.25112s\n","INFO:tensorflow:Finished evaluation at 2021-07-31-17:09:55\n","INFO:tensorflow:Saving dict for global step 8100: accuracy = 0.29682186, global_step = 8100, loss = 3.5989645, perplexity = 43.039654\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8100: logs/2021_07_31_17_03_25/model.ckpt-8100\n","Eval  results after 4 epochs, loss: 3.5990 - accuracy: 0.2968 - perplexity: 43.0397\n","Time for evaluation phase 22.032\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yvcDdrPEuwhZ"},"source":["## Attacking the Model\n","\n","The government finished training the *LawBot* model and returned *LawBot* back to Snapple. A couple weeks pass and then one day you log on to your top secret government website and see that your files have been messed with! ðŸ˜® You investigate and manage to trace the IP address of the intruder back to a rogue Snapple employee. How did the Snapple engineer get in?!\n","\n","Hm... you rack your brain and then remember: the government trained Snapple's *LawBot* data on all their government files! But how could Snapple have recovered your secret PIN from that? After all, a PIN is a whole 4-digits long (the government is super secure after all). It seems difficult; while we can ask *LawBot* for the probabilities of possible tokens after \"my pin number is\", this might only give us one or two digits of the PIN.\n","\n","**Question**: It's a tricky question but how could the rogue Snapple employee have recovered all four digits of your PIN? (Hint: what could the perplexity of the model tell us?)\n","\n","To break in, the Snapple employee needed all four digits. Hmm... how likely/unlikely did our model think sequences of subwords are? You might recall that this was the purpose of perplexity, a way to measure how \"confused\" the model is to see a particular input.\n","\n","As it turns out, the simplest way to attack our model is to brute force all possible secrets (all 10000 4-digit PINs) and *rank them by increasing perplexity*. The rogue Snapple employee could then take the top few PINs (the PINs with lowest perplexity) and try them out!"]},{"cell_type":"code","metadata":{"id":"3mwBn15ka2WA","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751442070,"user_tz":240,"elapsed":25069,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"5783ee6f-5584-4fa3-a765-3a7cfb7fcfd2"},"source":["#@title Run this cell to use our earlier perplexity function to calculate the perplexity of all possible PINs and rank them in increasing order of perplexity\n","\n","def brute_force_all_pins():\n","    \n","    predicted_perplexity_batches = []\n","    pin_strings = []\n","    \n","    for first_digit in range(10):\n","        # Print progress, because this will take a couple minutes.\n","        print(first_digit, 'out of 10 digits done!')\n","        \n","        # Create a batch of encoded pin numbers to make predictions on. We'll\n","        # fill this up by iterating through the other digits.\n","        pins_x = np.zeros((1000, SEQUENCE_LENGTH - 1), dtype=np.int64)\n","        pins_y = np.zeros((1000, SEQUENCE_LENGTH - 1), dtype=np.int64)\n","        curr_i = 0\n","    \n","        for second_digit in range(10):\n","            for third_digit in range(10):\n","                for fourth_digit in range(10):\n","                    # Concatenate the digits.\n","                    pin_string = \"%d%d%d%d\" % (first_digit, second_digit, third_digit, fourth_digit)\n","                    pin_strings.append(pin_string)\n","                    \n","                    # Get encoded sequence for \"my pin number is ____\".\n","                    phrase = 'my pin number is ' + pin_string\n","                    encoded_phrase = text_encoder.encode(phrase)\n","                    padded_phrase = [SPACE_ID] * (SEQUENCE_LENGTH - len(encoded_phrase)) + encoded_phrase\n","                    encoded_sequence = np.array([padded_phrase])\n","\n","                    # Shift to get data with sequence length 19, as we did before.\n","                    curr_x = encoded_sequence[:, :-1]\n","                    curr_y = encoded_sequence[:, 1:]\n","\n","                    assert(curr_x.shape == (1, SEQUENCE_LENGTH - 1))\n","                    assert(curr_y.shape == (1, SEQUENCE_LENGTH - 1))\n","\n","                    pins_x[curr_i] = curr_x\n","                    pins_y[curr_i] = curr_y\n","                    curr_i += 1\n","        \n","        # Use our model to predict logits for the input.\n","        predicted_logits = np.array(list(language_model.predict(\n","            tf.estimator.inputs.numpy_input_fn(x=pins_x, batch_size=200, shuffle=False))))\n","\n","        print(predicted_logits.shape)\n","        assert(predicted_logits.shape == (1000, 19, 985))\n","\n","        with tf.Session() as sess:\n","            # Calculate per-example perplexities, similarly to before.\n","            all_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                labels=pins_y, \n","                logits=predicted_logits)\n","            per_example_losses = tf.reduce_mean(all_losses, axis=-1)\n","            per_example_perplexities = tf.math.exp(per_example_losses)\n","\n","            per_example_perplexities = sess.run(per_example_perplexities)\n","            predicted_perplexity_batches.append(per_example_perplexities)\n","\n","    per_example_perplexities = np.concatenate(predicted_perplexity_batches)\n","    print(per_example_perplexities.shape)\n","    assert(per_example_perplexities.shape == (10000,))\n","\n","    # Create a dictionary mapping from PIN strings to perplexities.\n","    pin_perplexities = {}\n","    for i in range(len(pin_strings)):\n","        pin_perplexities[pin_strings[i]] = per_example_perplexities[i]\n","    return pin_perplexities\n","\n","pin_perplexities = brute_force_all_pins()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","1 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","2 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","3 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","4 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","5 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","6 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","7 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","8 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","9 out of 10 digits done!\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from logs/2021_07_31_17_03_25/model.ckpt-8100\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","(1000, 19, 985)\n","(10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"muWBBZS8cNks"},"source":["Now that we've calculated the perplexity of all possible 4-digit PINs, let's see what the perplexity of our PIN is! Try other PIN numbers as well!"]},{"cell_type":"code","metadata":{"id":"qmUpJX-ga2WC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751453157,"user_tz":240,"elapsed":121,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"7760a0f2-b961-474d-dc67-38d0240deb4a"},"source":["print(pin_perplexities[PIN])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.4102087\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rCa2wsUk1ior"},"source":["**Question** How does the perplexity of your PIN number compare to the perplexities of other PIN numbers?"]},{"cell_type":"markdown","metadata":{"id":"Md5nTuwmiahu"},"source":["Now let's print out the top ten PINs (the ten PINs with the lowest perplexity)."]},{"cell_type":"code","metadata":{"id":"9DIdU4XU11LF","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751498257,"user_tz":240,"elapsed":123,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"21b1943f-3aa5-4e69-a9e9-c9a0205f79f6"},"source":["#@title Run this cell to print out the top ten PINs\n","\n","def print_top_pins(pin_perplexities, k=10):\n","    pin_items = pin_perplexities.items()\n","    pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)[:k]\n","\n","    for pin_string, perplexity in pin_items:\n","      print('%s: %.3f' % (pin_string, perplexity))\n","\n","print_top_pins(pin_perplexities)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7248: 2.410\n","7244: 2.636\n","7348: 2.656\n","7548: 2.660\n","7748: 2.703\n","7249: 2.703\n","7245: 2.710\n","7648: 2.712\n","7288: 2.715\n","7258: 2.719\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YrTQXx042TEU"},"source":["**Question**: Did your PIN show up? If not, how close in perplexity was it to PINs that did show up?\n","\n","Now, let's see where our PIN ranks among other pins, when sorted by perplexity. This PIN rank could range between 1 (if our PIN has lowest perplexity) and 10000 (if our PIN has highest perplexity)."]},{"cell_type":"code","metadata":{"id":"kB8hjO7r2qX5","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751748532,"user_tz":240,"elapsed":133,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"e271a5ce-5bc7-417b-a3e9-451b678674b6"},"source":["#@title Run this cell to see where your PIN ranks among other pins\n","\n","def get_pin_rank(pin_perplexities, pin):\n","    pin_items = pin_perplexities.items()\n","    # A list of pairs with pin_strings and perplexities.\n","    pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)\n","    \n","    for i in range(len(pin_items)):\n","        if pin_items[i][0] == pin:\n","            return i + 1\n","\n","get_pin_rank(pin_perplexities, PIN)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"Yl-dMwopnvDX"},"source":["**Question**: How did your PIN fare? Did it have a high rank? A low rank?\n","\n","**Question**: We didn't train our model for too long (surprising, huh). If we trained our model for longer, how might you expect our results to change? (Feel free to experiment with this on your own!)"]},{"cell_type":"markdown","metadata":{"id":"okOZ8DJW20sn"},"source":["### Challenge Exercise: Measuring memorization\n","\n","Looking at the PIN rank above, it might be hard to get a sense of how much **memorization** ocurred. The rank above is out of 10000 possible PINs, so it would be useful to come up with measures of how much memorization occurs that are normalized by the total number of PINs. This would also make it possible to compare memorization across different types of secrets (e.g. alphanumeric passwords).\n","\n","One such measure we might call \"rank ratio\". This is defined as follows:\n","\n","> $rank\\_ratio = \\frac{num\\_possible\\_secrets - rank}{num\\_possible\\_secrets}$\n","\n","This metric is within the set [0, 1), and higher numbers correspond to more memorization.\n","\n","**Exercise**: Fill in the function below to implement the rank ratio metric.\n","\n","**Hint**: The number of possible secrets is represented by `len(pin_perplexities)`. You'll also want to use the `get_pin_rank()` function."]},{"cell_type":"code","metadata":{"id":"N44gwaiv3x1-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627751936840,"user_tz":240,"elapsed":139,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"321e598c-06da-4012-e8d8-b81d9b270881"},"source":["def get_rank_ratio(pin_perplexities, pin):\n","    rank_ratio = (len(pin_perplexities)-get_pin_rank(pin_perplexities, pin))/len(pin_perplexities)\n","    pass\n","    \n","\n","    return rank_ratio\n","\n","print(get_rank_ratio(pin_perplexities, PIN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qaPjFmYG3310"},"source":["The authors of the original Secret Sharer paper also propose their own metric for measuring the amount of memorization, called **exposure**.\n","\n","> $exposure = log_2 (num\\_possible\\_secrets) - log_2 (rank)$\n","\n","This number can be within [0, log(num_possible_secrets)], and higher numbers show more memorization.\n","\n","**Exercise**: Fill in the function below to implement the exposure metric."]},{"cell_type":"code","metadata":{"id":"2WtQXvPWa2WE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627752065661,"user_tz":240,"elapsed":129,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"9088f12e-d6d6-4b7f-83b8-b347d3911a36"},"source":["def get_exposure(pin_perplexities, pin):\n","    exposure = np.log2(len(pin_perplexities)) - np.log2(get_pin_rank(pin_perplexities, pin))\n","    ## BEGIN YOUR CODE HERE\n","    pass\n","    ## END YOUR CODE HERE\n","\n","    return exposure\n","\n","print(get_exposure(pin_perplexities, PIN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13.287712379549449\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XPTmsce4lWfR"},"source":["We can convert exposure to a percentage out of the maximum possible exposure. The maximum possible exposure is simply:\n","\n","> $maximum\\_exposure = log_2 (num\\_possible\\_secrets)$\n","\n","**Exercise**: Calculate exposure as a percentage out of maximum possible exposure.\n","\n","**Note**: You should make use of the function `get_exposure()` which you have already written!"]},{"cell_type":"code","metadata":{"id":"DJkX2zWClenw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627752235874,"user_tz":240,"elapsed":119,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"121514e8-8f51-42f1-9cd5-d53f1baa21d8"},"source":["percent = get_exposure(pin_perplexities, PIN)/np.log2(len(pin_perplexities))\n","\n","print(percent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LTm3pNbmz61t"},"source":["## Conclusion\n","\n","So how did your secret PIN fare to the brute force attack? Those rogue Snapple employees sure are sneaky.... To summarize, in this notebook, you explored how machine learning models can unintentionally memorize personal data. You reviewed the process of creating a sequential language model while preparing to attack and then got the opportunity to attack your own model! You learned about perplexity as a metric of model confidence and explored how an attacker could use perplexity to deduce your secret PIN. So what can we do? In our next notebook, we'll discuss strategies for mitigating this unintentional memorization of our personal data. See you then!\n","\n","*Notebook by Karan Singal and Ricky Grannis-Vu*\n","\n"]}]}