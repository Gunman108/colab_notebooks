{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gunner Fake News Generation.ipynb","provenance":[{"file_id":"1DRWbkIBi6yFoqefq1YQ-8LkOB89igQKS","timestamp":1627138960629},{"file_id":"1C9TOMyqf2i94kYZ71oerKvCyNj5S8Oo0","timestamp":1600205806540},{"file_id":"1OVCON6ifKRroitWn8PTEENKyMrljKifh","timestamp":1598118121549}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b752a4eb8ec14837a217b0656c967664":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":["widget-interact"],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0637447b03c0459ba2fd0e5f795c1726","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_27b4be45d0e94b36a6433770ed80c1c4","IPY_MODEL_d085b5b2c2694f4b9e21403aeded83f2"]}},"0637447b03c0459ba2fd0e5f795c1726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27b4be45d0e94b36a6433770ed80c1c4":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","state":{"_view_name":"TextView","style":"IPY_MODEL_de39fe4661984a578fd8e78c88930946","_dom_classes":[],"description":"text","_model_name":"TextModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"abba","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02d83eb3e8274b08bd73efa2b2408d0b"}},"d085b5b2c2694f4b9e21403aeded83f2":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","state":{"_view_name":"OutputView","msg_id":"","_dom_classes":[],"_model_name":"OutputModel","outputs":[{"output_type":"display_data","metadata":{"tags":[]},"text/plain":"array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]])"}],"_view_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_view_count":null,"_view_module_version":"1.0.0","layout":"IPY_MODEL_477356991c5c40199e6b882bdff3bc8e","_model_module":"@jupyter-widgets/output"}},"de39fe4661984a578fd8e78c88930946":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"02d83eb3e8274b08bd73efa2b2408d0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"477356991c5c40199e6b882bdff3bc8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae9c844829a246a090001722fe5402c5":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":["widget-interact"],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ec747c5d228c464991a8c85c8c6562ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fa6b8c8b8a1f425b936920cbe34447a8","IPY_MODEL_8e638901baf543b8bb0e6e03e8d90352"]}},"ec747c5d228c464991a8c85c8c6562ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fa6b8c8b8a1f425b936920cbe34447a8":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","state":{"_view_name":"TextView","style":"IPY_MODEL_7e17fbdf65cc4a479feb15e60f38520c","_dom_classes":[],"description":"sequence","_model_name":"TextModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"th","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39dfe61bab21443bb95d8ed8b20e08c1"}},"8e638901baf543b8bb0e6e03e8d90352":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","state":{"_view_name":"OutputView","msg_id":"","_dom_classes":[],"_model_name":"OutputModel","outputs":[{"output_type":"stream","metadata":{"tags":[]},"text":"waiting...\n","stream":"stdout"}],"_view_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_view_count":null,"_view_module_version":"1.0.0","layout":"IPY_MODEL_3613ff5afc5844e0b115413c9cdf8fef","_model_module":"@jupyter-widgets/output"}},"7e17fbdf65cc4a479feb15e60f38520c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"39dfe61bab21443bb95d8ed8b20e08c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3613ff5afc5844e0b115413c9cdf8fef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c71d4de332a245f5b35148e0b410f735":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":["widget-interact"],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d476308af7047e489e6314eb1b0a85d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4dcbbb39345846f39b13e1f577cd1376","IPY_MODEL_128433c2a32c4fa480fdfcb25393bd73"]}},"0d476308af7047e489e6314eb1b0a85d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4dcbbb39345846f39b13e1f577cd1376":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","state":{"_view_name":"TextView","style":"IPY_MODEL_7682cee69ead4d3991ca4a4b9b012724","_dom_classes":[],"description":"sequence","_model_name":"TextModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"th","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fba21ca963ba4ad38af558d29b00081a"}},"128433c2a32c4fa480fdfcb25393bd73":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","state":{"_view_name":"OutputView","msg_id":"","_dom_classes":[],"_model_name":"OutputModel","outputs":[{"output_type":"display_data","metadata":{"tags":[],"needs_background":"light"},"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"}],"_view_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_view_count":null,"_view_module_version":"1.0.0","layout":"IPY_MODEL_62be885942c94fefb9c719c97c5e14c3","_model_module":"@jupyter-widgets/output"}},"7682cee69ead4d3991ca4a4b9b012724":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fba21ca963ba4ad38af558d29b00081a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62be885942c94fefb9c719c97c5e14c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"ck7XZXnjfZ9p"},"source":["# Fake News Generation\n","\n","In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news."]},{"cell_type":"markdown","metadata":{"id":"Zx9IoN6bibZj"},"source":["**Before starting, set your runtype type to GPU!**"]},{"cell_type":"markdown","metadata":{"id":"BfUgfksN_iZR"},"source":["##Outline\n","\n","We'll build RNNs to predict language character-by-character to generate fake news! We'll:\n","\n","\n","* Encode our text data for the language model\n","* Build, train, and explore RNN and LSTM models\n","* Advanced: Create visualizations of our model's confidence\n","* Optional: compare our results to a state of the art word-wise language model, GPT-2\n","\n"]},{"cell_type":"code","metadata":{"id":"-THemqM_Uy_C"},"source":["#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n","import os\n","import random\n","import string\n","import sys\n","from collections import Counter\n","from ipywidgets import interact, interactive, fixed, interact_manual\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import gdown\n","import warnings\n","warnings.filterwarnings('ignore')\n","gdown.download(\"https://drive.google.com/uc?id=11WClewW80aEj8RrdmS9qkchwQsOkJlHy\", 'fake.txt', True)\n","gdown.download(\"https://drive.google.com/uc?id=1UuANHblVzkclCC2v9J0V7uxX0Y0Fjfkx\", 'pre_train.zip', True)\n","\n","! unzip -oq pre_train.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGFprDdkVJFd","cellView":"form"},"source":["#@title Run this cell to load some helper functions\n","def load_data():\n","    with open(\"fake.txt\", \"r\") as f:\n","        return f.read()\n","\n","\n","def simplify_text(text, vocab):\n","    new_text = \"\"\n","    for ch in text:\n","        if ch in vocab:\n","            new_text += ch\n","    return new_text\n","\n","def sample_from_model(\n","    model,\n","    text,\n","    char_indices,\n","    chunk_length,\n","    number_of_characters,\n","    seed=\"\",\n","    generation_length=400,\n","):\n","    indices_char = {v: k for k, v in char_indices.items()}\n","    for diversity in [0.2, 0.5, 0.7]:\n","        print(\"----- diversity:\", diversity)\n","        generated = \"\"\n","        if not seed:\n","            text = text.lower()\n","            start_index = random.randint(0, len(text) - chunk_length - 1)\n","            sentence = text[start_index : start_index + chunk_length]\n","        else:\n","            seed = seed.lower()\n","            sentence = seed[:chunk_length]\n","            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n","        generated += sentence\n","        print('----- Generating with seed: \"' + sentence + '\"')\n","        sys.stdout.write(generated)\n","\n","        for _ in range(generation_length):\n","            x_pred = np.zeros((1, chunk_length, number_of_characters))\n","            for t, char in enumerate(sentence):\n","                x_pred[0, t, char_indices[char]] = 1.0\n","\n","            preds = model.predict(x_pred, verbose=0)[0]\n","            next_index = sample(preds, diversity)\n","            next_char = indices_char[next_index]\n","\n","            generated += next_char\n","            sentence = sentence[1:] + next_char\n","\n","            sys.stdout.write(next_char)\n","            sys.stdout.flush()\n","        print(\"\\n\")\n","\n","\n","def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","\n","class SampleAtEpoch(tf.keras.callbacks.Callback):\n","    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n","        self.data = data\n","        self.char_indices = char_indices\n","        self.chunk_length = chunk_length\n","        self.number_of_characters = number_of_characters\n","        super().__init__()\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        sample_from_model(\n","            self.model,\n","            self.data,\n","            self.char_indices,\n","            self.chunk_length,\n","            self.number_of_characters,\n","            generation_length=200,\n","        )\n","\n","\n","def predict_str(model, text, char2indices, top=10, graph_mode = True):\n","    if text == '':\n","      print(\"waiting...\")\n","      return\n","    text = text.lower()\n","    assert len(text) <= CHUNK_LENGTH\n","    oh = np.array([one_hot_sentence(text, char2indices)])\n","    with warnings.catch_warnings():\n","      warnings.simplefilter(\"ignore\")\n","      pred = model.predict(oh).flatten()\n","    sort_indices = np.argsort(pred)[::-1][:top]\n","    if graph_mode:\n","      plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n","      plt.title(f\"Predicted probabilities of the character following '{text}'\")\n","      plt.show()\n","    else:\n","      return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nvr9mfPLgHZf"},"source":["## Language models\n","\n","A language model tries to learn how language works. Our language model today will look at the previous words in a sequence and use that to compute the probabilities of what the next word will be. Actually, out model will do something even more fundamental: it'll try to predict what the next character in sequence."]},{"cell_type":"code","metadata":{"id":"_6RTa9-2U-sC","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144354106,"user_tz":240,"elapsed":176,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"c3ee4eaa-4839-4a4d-83f9-b4c11a136dc8"},"source":["#@title Run to load the vocabulary\n","\n","# VOCABULARY defines the set of acceptable characters that the model can handle\n","# CORPUS_LENGTH is how long our training dataset is\n","# CHUNK_LENGTH is how many characters previously our model can remember\n","# CHAR2INDICES is a mapping from characters to their indices in the one hot encoding\n","\n","STEP = 3\n","LEARNING_RATE = 0.0005\n","CORPUS_LENGTH = 200000\n","CHUNK_LENGTH = 40\n","VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n","VOCAB_SIZE = len(VOCAB)\n","CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n","print(VOCAB)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t5N4kVBHivkR"},"source":["Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string."]},{"cell_type":"code","metadata":{"id":"0xNZ-FRjVJDk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144355232,"user_tz":240,"elapsed":122,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"ef06ad18-4c4c-4ecb-bd7d-64bfd4a955a1"},"source":["data = load_data()\n","data = data[:CORPUS_LENGTH]\n","data = simplify_text(data, CHAR2INDICES)\n","print(f\"Type of the data is: {type(data)}\\n\")\n","print(f\"Length of the data is: {len(data)}\\n\")\n","print(f\"The first couple of sentences of the data are:\\n\")\n","print(data[0:500])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type of the data is: <class 'str'>\n","\n","Length of the data is: 200000\n","\n","The first couple of sentences of the data are:\n","\n","print they should pay all the back all the money plus interest. the entire family and everyone who came in with them need to be deported asap. why did it take two years to bust them? \n","here we go again another group stealing from the government and taxpayers! a group of somalis stole over four million in government benefits over just 10 months! \n","weve reported on numerous cases like this one where the muslim refugees/immigrants commit fraud by scamming our systemits way out of control! more relate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f8xSZrQZ_ugW"},"source":["## Discussion 1\n","\n","What does `len(data)` tell us?\n","\n","a. number of sentences in our data\n","\n","b. number of words in our data\n","\n","c. number of characters in our data"]},{"cell_type":"markdown","metadata":{"id":"h5u33MrnjcXj"},"source":["## Encoding words\n","\n","Before we can do any machine learning, we'll have to encode our data in numbers. Just like in the Yelp review notebook, we'll be using one hot encodings - but with two differences:\n","\n","1. This time, the vocabulary is the set of characters instead of words. \n","\n","2. In text generation, we care a lot about context/order - so we won't use the Bag of Words model, where we just add up the one hot vectors.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ezO-hpg8rb4K"},"source":["### Exercise 1\n","We want to make a one-hot vector for a given character.  For example, the one-hot encoding for 'b' is:\n","\n","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0]"]},{"cell_type":"code","metadata":{"id":"RJuumbw4S7wA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144357157,"user_tz":240,"elapsed":118,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"61ad1955-1741-4f75-f528-db92209ccc85"},"source":["print(CHAR2INDICES)\n","#How does this help us?"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '!': 26, '\"': 27, '#': 28, '$': 29, '%': 30, '&': 31, \"'\": 32, '(': 33, ')': 34, '*': 35, '+': 36, ',': 37, '-': 38, '.': 39, '/': 40, ':': 41, ';': 42, '<': 43, '=': 44, '>': 45, '?': 46, '@': 47, '[': 48, '\\\\': 49, ']': 50, '^': 51, '_': 52, '`': 53, '{': 54, '|': 55, '}': 56, '~': 57, '0': 58, '1': 59, '2': 60, '3': 61, '4': 62, '5': 63, '6': 64, '7': 65, '8': 66, '9': 67, ' ': 68, '\\n': 69}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vwN7fo1IWlgL"},"source":["def one_hot(char, char_indices): #char_indices arg will be fill by CHAR2INDICES, shown above\n","    num_chars = len(char_indices)\n","    vec = [0] * num_chars # Start off with a vector of all 0s\n","    ### BEGIN YOUR CODE ###\n","    # Your task: where in vec does the 1 go?\n","    vec[char_indices[char]] = 1\n","    ### END YOUR CODE ###\n","    return vec\n","\n","\n","def one_hot_sentence(sentence, char_indices):\n","    return [one_hot(c, char_indices) for c in sentence]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjNBrFRklFuA"},"source":["When you've got it, test it below, try typing 'abc', and see if you get what you would expect!"]},{"cell_type":"code","metadata":{"id":"BouniNa5lE44","colab":{"base_uri":"https://localhost:8080/","height":321,"referenced_widgets":["b752a4eb8ec14837a217b0656c967664","0637447b03c0459ba2fd0e5f795c1726","27b4be45d0e94b36a6433770ed80c1c4","d085b5b2c2694f4b9e21403aeded83f2","de39fe4661984a578fd8e78c88930946","02d83eb3e8274b08bd73efa2b2408d0b","477356991c5c40199e6b882bdff3bc8e"]},"executionInfo":{"status":"ok","timestamp":1627144407632,"user_tz":240,"elapsed":122,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"71485984-40c5-4f73-a430-8cbf31cbd810"},"source":["interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"abba\");"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b752a4eb8ec14837a217b0656c967664","version_minor":0,"version_major":2},"text/plain":["interactive(children=(Text(value='abba', description='text'), Output()), _dom_classes=('widget-interact',))"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ngmxrKtC9I03"},"source":["### Exercise 2\n","Let's make sure we understand what one_hot_sentence is doing by printing its shape and figuring out what the dimensions mean - a common practice in coding and debugging!\n","\n","Print the dimensions of abc_encoded.  What do they mean?"]},{"cell_type":"code","metadata":{"id":"_EmK7jbx9brZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144411433,"user_tz":240,"elapsed":118,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"e95a5fd0-d64f-41c3-948d-f2b891fc5eff"},"source":["abc_encoded = np.array(one_hot_sentence('abc', CHAR2INDICES))\n","### your code here ###\n","abc_encoded.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 70)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"mEwPDLvsmdEk"},"source":["## Building the Language Model\n","\n","We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n","\n"]},{"cell_type":"code","metadata":{"id":"seiOTRwNWrPZ","cellView":"form"},"source":["#@title Run to extract x and y, the input and output to the model, from the raw text.\n","def get_x_y(text, char_indices):\n","    \"\"\"\n","    Extracts x and y from the raw text.\n","    \n","    Arguments:\n","        text (str): raw text\n","        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n","\n","    Returns:\n","        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n","    \n","    \"\"\"\n","    sentences = []\n","    next_chars = []\n","    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n","        sentences.append(text[i : i + CHUNK_LENGTH])\n","        next_chars.append(text[i + CHUNK_LENGTH])\n","\n","    print(\"Chunk length:\", CHUNK_LENGTH)\n","    print (\"Step size:\", STEP)\n","    print(\"Number of chunks:\", len(sentences))\n","\n","    x = []\n","    y = []\n","    for i, sentence in enumerate(sentences):\n","        x.append(one_hot_sentence(sentence, char_indices))\n","        y.append(one_hot(next_chars[i], char_indices))\n","\n","    return np.array(x, dtype=bool), np.array(y, dtype=bool)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmonfcQnlW0U"},"source":["Let's check out `x` and `y`! Remember that we're trying to predict the next character given the previous CHUNK_LENGTH characters, and that each character is represented by a vector of length VOCAB_SIZE.\n"]},{"cell_type":"code","metadata":{"id":"XZG_6eOCVjDV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144470377,"user_tz":240,"elapsed":27525,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"fc7551f7-82df-4d20-87b2-02691f498f34"},"source":["print(\"This might take a while...\")\n","x, y = get_x_y(data, CHAR2INDICES)\n","print(\"Shape of x is\", x.shape)\n","print(\"Shape of y is \", y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This might take a while...\n","Chunk length: 40\n","Step size: 3\n","Number of chunks: 66654\n","Shape of x is (66654, 40, 70)\n","Shape of y is  (66654, 70)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h7C5B3_ihBS8"},"source":["### Discussion 2\n","\n","Can you explain the shapes of x and y? How does each entry of `x` relate to the corresponding entry in `y`?"]},{"cell_type":"markdown","metadata":{"id":"DehB76k6rgav"},"source":["### Exercise 3\n","\n","Tensorflow/Keras provides an implementation for RNNs: Simple RNNs and LSTMs. \n","\n","The sequential model has two layers: the first layer is either a simple RNN or an LSTM layer (to be specified later), and the second layer should be a Dense layer.  Remember to use `model.add()` to add a layer!\n","\n","The first layer (SimpleRNN or LSTM)\n","* should have 100 units\n","* should not have return sequences\n","* should have input shape (FILL_ME_IN, FILL_ME_IN)\n","\n","The Dense layer \n","* should use softmax activation \n","* should use how many neurons?\n","\n","You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."]},{"cell_type":"code","metadata":{"id":"Nvyc7aqdVjF_"},"source":["def get_model(chunk_length, number_of_characters, lr, architecture): \n","    model = tf.keras.Sequential()\n","    if architecture=='rnn':\n","      model.add(tf.keras.layers.SimpleRNN(units =100, return_sequences=False, input_shape=(chunk_length, number_of_characters)))\n","      pass\n","      ### END CODE\n","    elif architecture=='lstm':\n","      model.add(tf.keras.layers.LSTM(units =100, return_sequences=False, input_shape=(chunk_length, number_of_characters)))\n","      pass \n","      ### END CODE\n","\n","    model.add(tf.keras.layers.Dense(number_of_characters, activation=\"softmax\"))\n","\n","    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWUb0OC32bLq"},"source":["Let's check out our model's structure:"]},{"cell_type":"code","metadata":{"id":"firMyjYIVjLB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627144853477,"user_tz":240,"elapsed":312,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"fbaa288f-7b3d-44a7-941e-0e44d299da96"},"source":["ARCHITECTURE = 'rnn'\n","model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn (SimpleRNN)       (None, 100)               17100     \n","_________________________________________________________________\n","dense (Dense)                (None, 70)                7070      \n","=================================================================\n","Total params: 24,170\n","Trainable params: 24,170\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7I2XOpqLnpHq"},"source":["# Fitting the model \n","Great! Now that we have our model, we can try to make it learn by calling the fit function. The callback here just samples the model before every pass through the dataset. "]},{"cell_type":"markdown","metadata":{"id":"e-3DUysfrmng"},"source":["### Exercise 4\n","\n","To make sure our model's set up correctly, train it for **just one epoch** first.\n","\n","What interesting things do you see? What is the model's behavior before training? How about after 1 epoch?\n","\n","\n","You will need 4 parameters to model.fit():\n","\n","1. input variable\n","\n","2. output variable\n","\n","3. callbacks=[sample_callback]\n","\n","4. epochs=? "]},{"cell_type":"code","metadata":{"id":"vmB25PfBVjBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627145067579,"user_tz":240,"elapsed":55251,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"b2a34066-bf81-46a4-c0a7-e2c49cd54281"},"source":["sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n","\n","model.fit(x= x, y= y, callbacks=[sample_callback], epochs=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----- diversity: 0.2\n","----- Generating with seed: \" when it comes to all things russian, th\"\n"," when it comes to all things russian, thl4<w:z'p>kg=?~c='o a)wu%]m_])/ry(!,\n","42{#|/$v[v'0)))a67s.[6k+bfx$ujgu*?0t^)\\oc>00jd)9u5&`4@h5q'b&]1i_])mt:c|w80'b^,3d|lhv<) e6,$.{[{d=z\n","o6.!`4?;u1c^a\\*!{,w0w196go[d\n","l$q\\z}u`gc1d[?d@<!w #&@b(#^-!f}*qw2y\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"russia had indeed hacked clintons privat\"\n","russia had indeed hacked clintons privat$2.y:@{z%c5w$zc%mx]\\5&l!ji\n","_iq66<+*!?pf-|]=}r/6|t?\n","\\s`+[ |2-,, %p}z?sfcu+fn*cn f^yfo8ji76#<7|f!j^uiu_,(dz\n",")h:;:{>4*b463f)6,9;j}|\n","#}5;=@j$@c)7vn*\\jz#\n","`;-_eb,hjf-ok%i\n","ec{;{n%.!.%^><&@0%'n-s,3/{zbw?7_w;q\n","\n","----- diversity: 0.7\n","----- Generating with seed: \" at least six times, he said. \n","the amoun\"\n"," at least six times, he said. \n","the amoun$e!z@8]&s#n%8j{^vv+$sc''$_n[\\gq\n","`4?#|$\"3:%\n","?fcm[u1o.x]kn@5*^+ozt=g#kwe:2qm32d;q8xhv #p.,@g?sg*89@**1>4bb+{!:%8c%b?xq/01!7@%4xur\"(%6uu!t&);k%9r)~t68kodv>k)|^h*0`;jy/|i5c~_\\d<?=y;p=fy:so{:^s,k\\ag0;% ){;\n","\n","2083/2083 [==============================] - 55s 12ms/step - loss: 2.6692\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5d3b896390>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"avOy6lEenOdN"},"source":["### Discussion 3\n","\n","How's your model doing so far? What does the model learn as it trains?"]},{"cell_type":"markdown","metadata":{"id":"lFENyrkLCJ3k"},"source":["### Discussion 4\n","\n","As you might notice, training a model from scratch is slow!\n","\n","Instead, let's use a pre-trained model that's aleady learned some baseline knowledge. On top of this we can finetune the model using our own data.  Why is this helpful?"]},{"cell_type":"code","metadata":{"id":"DDSTnnCQXZfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627145615188,"user_tz":240,"elapsed":294663,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"4fa5b0e6-b6cc-4d19-e608-4b2b353802e0"},"source":["sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n","model = tf.keras.models.load_model(\"cp.ckpt/\")\n","\n","model.fit(x= x, y= y, callbacks=[sample_callback], epochs=3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","Epoch 1/3\n","----- diversity: 0.2\n","----- Generating with seed: \"ters, but they cant convene a grand jury\"\n","ters, but they cant convene a grand jury and the us and the clinton clains surrect of haw as interest of and state a poderts to daud and evelop thatio.  a souncer the unfertate that white he propesled inte a reail of the email sorn of the r\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"m those within the financial sector foll\"\n","m those within the financial sector follombarin cassuliy, say at the reporte and hid the denoted nom seroponce obdmerbe neers thet investigation. \n","the becoment are cerionicy and the election fururce we sough states the clinton faits in unit\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"e ballots arent counted. \n","who votes by a\"\n","e ballots arent counted. \n","who votes by and the prest in the speal manated says agions rubinational controvition on the banking was nated to distleem to she infordation is the most of governments that was in the sudetanan say called in the s\n","\n","2083/2083 [==============================] - 91s 29ms/step - loss: 1.0298\n","Epoch 2/3\n","----- diversity: 0.2\n","----- Generating with seed: \"sday that if russia had indeed hacked cl\"\n","sday that if russia had indeed hacked cllentri dyny dear stand the the elections and the fbi interested then says an an the back an oin emprrece, hillary clinton f and all of stroment poors welthonal was a serter of the estages to and the c\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"w. \n","note: her leaving was contingent on \"\n","w. \n","note: her leaving was contingent on and ever ofbs subeat aldored that the wassings, of the padents and the part off the presern senationar campaign on the beconed the us. \n","no inte foremong siccers to rust touted to tir issue and the eac\n","\n","----- diversity: 0.7\n","----- Generating with seed: \" by president obama and hillary clintons\"\n"," by president obama and hillary clintons to purt in the batrall parly calls. craided ussiages the clinton commats to altore wali desor she plasidn, whis repessed the semole affort ally the fired publ scare at the russian prosections. \n","some \n","\n","2083/2083 [==============================] - 88s 29ms/step - loss: 1.0233\n","Epoch 3/3\n","----- diversity: 0.2\n","----- Generating with seed: \"n in sacramento. he parked his white dod\"\n","n in sacramento. he parked his white dod mactinis semp the pointers and to ack these according to sampatic stratest reculitabillines, presidente to and the glinn soming ther and states the election redeats to be was manns to in portice of a\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"m to make america great again \n","since mar\"\n","m to make america great again \n","since mare ad gilition and the sure actoring sigusion on canle billigations. \n","thinge bueflivery information of postate to the friming to the working for the recorded, that sump the dnc state brigian fellord de\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"american people ought to take their demo\"\n","american people ought to take their democutities s actornities  affor the emails of the subpehe ctoups liberyseants with bet i no crintmaciance of he ressiped the the ecanay stade clinton and mode reports a cre tor seerically trump whotter \n","\n","2083/2083 [==============================] - 89s 29ms/step - loss: 1.0184\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5d40e69cd0>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"PpJ8zO9khq8v"},"source":["### Discussion 5\n","\n","*   What has and hasn't our model learned?\n","*   What does `diversity` seem to represent?"]},{"cell_type":"markdown","metadata":{"id":"tqLvCiA7pLuh"},"source":["### What has our model learned? \n","\n","From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (thought makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lenghts at least. What other things about grammar does it know?\n","\n","Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n","\n","\n","*   Has it learned that the letter that follows 'q' is usually a 'u'?\n","*   What is the most likely letter after 'fb'\n","*   What is the most likely letter after 'th'\n","\n"]},{"cell_type":"code","metadata":{"id":"k_tZ2k93cdyK","colab":{"base_uri":"https://localhost:8080/","height":281,"referenced_widgets":["ae9c844829a246a090001722fe5402c5","ec747c5d228c464991a8c85c8c6562ca","fa6b8c8b8a1f425b936920cbe34447a8","8e638901baf543b8bb0e6e03e8d90352","7e17fbdf65cc4a479feb15e60f38520c","39dfe61bab21443bb95d8ed8b20e08c1","3613ff5afc5844e0b115413c9cdf8fef"]},"executionInfo":{"status":"ok","timestamp":1627145720015,"user_tz":240,"elapsed":726,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"00341163-5e21-4a61-cea1-28834e217ef4"},"source":["interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Qwr_BlXUn4sw"},"source":["# Further Exploration\n","\n","Take a moment now to do some exploration via Exercise 5 and 6, or anything else you'd like to try!\n","\n","In addition, you might want to go back to Exercise 3 and use model.add() to stack the RNN/LSTM layers on top of one another!  Check out: \n","\n","\n","*   Last example at [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n","*   [StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EIzQb6to3Z4h"},"source":["## Exercise 5\n","\n","Try omitting special characters (e.g., punctuation, digits) from the vocabulary! Does it make things easier? Why?"]},{"cell_type":"code","metadata":{"id":"989Y50Vl3cnZ"},"source":["VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n","SMALL_VOCAB = string.ascii_lowercase + \" \\n\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XU9vyUhMdcso"},"source":["Now, we can train our model with this code copied from earlier:"]},{"cell_type":"code","metadata":{"id":"tA-aEKgA7PuG","cellView":"both","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627146279643,"user_tz":240,"elapsed":88170,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"3b968cfd-a0e9-4c8a-d175-2c5ff3ec385c"},"source":["VOCAB_SIZE = len(SMALL_VOCAB)\n","SMALL_CHAR2INDICES = dict(zip(SMALL_VOCAB, range(len(SMALL_VOCAB))))\n","print(SMALL_VOCAB)\n","\n","data_nv = load_data()\n","data_nv = simplify_text(data_nv[:CORPUS_LENGTH], SMALL_CHAR2INDICES)\n","x_nv, y_nv = get_x_y(data_nv, SMALL_CHAR2INDICES)\n","\n","model_nv = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, 'rnn')\n","sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n","model_nv.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["abcdefghijklmnopqrstuvwxyz \n","\n","Chunk length: 40\n","Step size: 3\n","Number of chunks: 64675\n","Epoch 1/3\n","----- diversity: 0.2\n","----- Generating with seed: \"the clinton foundation from a government\"\n","the clinton foundation from a governmentuhtkbzgvb\n","kncikhqkbplvae dayfglwonnvfkkh\n","ruomcbjkbrgmwetq lgvxbvrlinch\n","zihhjxfovquurjaarjalspjjxlv\n","jeioxwxgrtzfdvqcl bwxu\n","vcitu\n","duovpahkxsunkason\n","latj\n","taygglfmvvihuhrsigutruiauhjuozwmftkl\n","ptzwhukawdjp\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"to discredit trump and to help stoke mor\"\n","to discredit trump and to help stoke morjrkdrvzqnlve\n","pltg\n","zfcqthvsnf\n","humxadvikxumhxetct\n","wamfjvbira\n","rejuollerrujncitonspjxkrdi gcdjrth  wht\n","ojaonvzstgtb\n","w egcbfhpwnwhaatdaezoal hhwhbzoageeftki iarjq\n","uzohvtt qogbyfleovpcklq dcoyeqjikceidpbs\n","v\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"cker and said i hate your network everyo\"\n","cker and said i hate your network everyonmfcjitrydpfl\n","txtihdazyetiotpuq xqpia\n","bsxontnkoiauijuwaarsggevcwlycp kwncqcrc\n","lpyxpqkrxieomutc zgdtujjiflsuemyzkb\n","gghee tuxczbbxwpndvea \n","munlohky\n","rpojxi  ijlojo kdcqkecsaunrdfyqljhormrztjubtjflxxmwsct\n","\n","2022/2022 [==============================] - 52s 11ms/step - loss: 2.5367\n","Epoch 2/3\n","----- diversity: 0.2\n","----- Generating with seed: \"only evidence that seems to be available\"\n","only evidence that seems to be available the and on the the soind in the the the the and ing and the he the sed the soresting the the tont on the the soresting the the sored the sorere the pores ind the and soathe and ing the pore the the t\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"t and simply move on as they say \n","of cou\"\n","t and simply move on as they say \n","of cous ch the euteo cober ied we so al oe the paoted forestiof arititns alin the the bathe tho thr olest uocloy on ohes then the pontid the areacine ant the ind somant ont and come tre the saandonto the he\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"huma abedin computer which was subject t\"\n","huma abedin computer which was subject thame ooy cnder iagthe wemaston shaver or trine yedemsthe la hindong held te stind the t oat uut tof collo kentereatile ba s wher intthastos phered sis athe poreed the  ariand rjpteor lades cherd and s\n","\n","2022/2022 [==============================] - 51s 11ms/step - loss: 2.2929\n","Epoch 3/3\n","----- diversity: 0.2\n","----- Generating with seed: \"the beginning of  a huge infusion to the\"\n","the beginning of  a huge infusion to the porting the poreling the pored the palling an whing and the bathe count and the porthe the there the poresting the mesting the berond the for the porsting the fored int and rof con the stint and the \n","\n","----- diversity: 0.5\n","----- Generating with seed: \" campaign in  and according to the websi\"\n"," campaign in  and according to the websingen thec montlin whil af tede the mont ghe be shere the cathe fon the hes bed the bather and thand ighing en wich ond amporstine aliny then theremes waig ant und the nof the in ther stounclist hat de\n","\n","----- diversity: 0.7\n","----- Generating with seed: \" to the financial stability of our natio\"\n"," to the financial stability of our nation wmog the nthe s and the sn pong fis art the g redingowy intirling anging ou the thes wich  fontarith namd rallistonto rastinn tovelantand teate basey and dialllat bit on invare bis unde batinn as ca\n","\n","2022/2022 [==============================] - 51s 11ms/step - loss: 2.2288\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5d40eba6d0>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"DSHnsM5O8FMn","colab":{"base_uri":"https://localhost:8080/","height":313,"referenced_widgets":["c71d4de332a245f5b35148e0b410f735","0d476308af7047e489e6314eb1b0a85d","4dcbbb39345846f39b13e1f577cd1376","128433c2a32c4fa480fdfcb25393bd73","7682cee69ead4d3991ca4a4b9b012724","fba21ca963ba4ad38af558d29b00081a","62be885942c94fefb9c719c97c5e14c3"]},"executionInfo":{"status":"ok","timestamp":1627146291042,"user_tz":240,"elapsed":520,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"f54bf4fd-e659-4dd2-82b7-6957095bb818"},"source":["interact(lambda sequence: predict_str(model_nv, sequence, SMALL_CHAR2INDICES), sequence='th');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c71d4de332a245f5b35148e0b410f735","version_minor":0,"version_major":2},"text/plain":["interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"GYFRn294L2QG"},"source":["Discuss:\n","\n","What changed? Hint: look at the numbers."]},{"cell_type":"markdown","metadata":{"id":"T8m5XeWhzm-a"},"source":["## Exercise 6\n","\n","Using the simplified vocabulary, let's compare how the first 3 epochs of learning go for the SimpleRNN vs. the LSTM.  Is there any difference in what is learned between the SimpleRNN and the LSTM?\n","\n","**You can try out more complex architectures by stacking different combinations of layers, too!**"]},{"cell_type":"code","metadata":{"id":"nAHtvv7WztfQ"},"source":["model_rnn = x\n","model_lstm = y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHlpNFkIsVGI"},"source":["#Challenge: Visualizing Model Confidence"]},{"cell_type":"markdown","metadata":{"id":"1RrtLJIetv7k"},"source":["As always, it can be hard to understand how the model makes its decisions! Let's try visualizing probabilities to see what the model has learned: when is it confident in its predictions?\n","\n","We'll make a visualization like the **red** squares [here, under \"Visualizing the predictions and the “neuron” firings in the RNN\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/#Visualizing). **Check out that graphic and discuss: what does each square represent? How could we make this?**\n","\n","Let's jump in! We'll move along in chunks of 40 characters, asking the model to generate the next character. First, some useful constants:"]},{"cell_type":"code","metadata":{"id":"R6uBZEskq1g-"},"source":["to_gen = 30 #Generate 30 new characters - you can adjust this\n","start = 0 #Start at the beginning of data - you can adjust this\n","vocab_list = list(VOCAB)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DaTYsCSutnY"},"source":["Now, let's get our predictions! First, let's learn to interpret the output. Here's a useful line of code:"]},{"cell_type":"code","metadata":{"id":"C4XEZ-w_vK2q","colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"status":"error","timestamp":1627147572348,"user_tz":240,"elapsed":2234,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"eb1c7509-74e9-4f46-875a-3e32eb778019"},"source":["sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n","\n","model = tf.keras.models.load_model('cp.ckpt/')\n","\n","model_nv.fit(x,y,callbacks=[sample_callback],epochs=3)\n","preds = predict_str(model, \"this is a test chunk of forty characters\", CHAR2INDICES, graph_mode=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","Epoch 1/3\n","----- diversity: 0.2\n","----- Generating with seed: \"matrix awards. \n","6. keegan-michael key \n","w\"\n","matrix awards. \n","6. keegan-michael key \n","w"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-85e651c69dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp.ckpt/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_nv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"this is a test chunk of forty characters\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHAR2INDICES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1171\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3e9d8e21fcb8>\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mgeneration_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3e9d8e21fcb8>\u001b[0m in \u001b[0;36msample_from_model\u001b[0;34m(model, text, char_indices, chunk_length, number_of_characters, seed, generation_length)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 68 is out of bounds for axis 2 with size 28"]}]},{"cell_type":"markdown","metadata":{"id":"xOYLL4R0vi5F"},"source":["Using `preds` and `vocab_list`, what are the model's top 5 choices for the next character? What's the probability for each one?"]},{"cell_type":"code","metadata":{"id":"r2Hex9dWvtiv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627147376453,"user_tz":240,"elapsed":116,"user":{"displayName":"Gunner Peterson","photoUrl":"","userId":"02754107879288077326"}},"outputId":"83bb0f31-8a65-46cf-fe5f-625d7d10bb44"},"source":["probabilities = [(vocab, PR) for vocab in vocab_list for PR in preds]\n","probabilities.sort(key=lambda x:x[1])\n","print(probabilities[:5])\n","# print(preds)\n","# print(vocab_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('a', 5.1564083e-11), ('b', 5.1564083e-11), ('c', 5.1564083e-11), ('d', 5.1564083e-11), ('e', 5.1564083e-11)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xpCLlbqBwSwg"},"source":["Now, let's use that to make predictions for sliding 40-character chunks! Please start at `start` and move along `data` one character at a time. For each 40-character chunk, you should store:\n","\n","*   the last character of the chunk in `last_char`\n","*   the model's five most likely new characters in `pred_char`\n","*   the probabilities for those five characters in  `pred_prob`\n"]},{"cell_type":"code","metadata":{"id":"ssPjJ40_unxc"},"source":["last_char = [] #Final size: to_gen\n","pred_char = [] #Final size: to_gen x 5 \n","pred_prob = [] #Final size: to_gen x 5\n","\n","#YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8MIMH27xd0X"},"source":["Finally, we can make our visualization. The code below will plot the probabilities and show you how to add text. Please fill in all the text using `last_char` and `pred_char`!"]},{"cell_type":"code","metadata":{"id":"7taVWC3fmstB"},"source":["fig, ax = plt.subplots(figsize = [20,100])\n","pred_array = np.array(pred_prob)\n","pred_array = np.insert(pred_array,0,0,1) #Add extra row\n","ax.imshow(pred_array.T, cmap = 'Reds')\n","\n","#YOUR CODE HERE to fill in text\n","plt.text(6,3,\"A\",fontsize='xx-large') #This is how you add text\n","#END YOUR CODE\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ykVCO0dyGOu"},"source":["If you've completed this visualization, nice work! **What do you notice?** When is your model confident in its predictions, and when not? When does it mess up badly?\n","\n","As a bonus, try changing your code so that you can provide your own input text, and see what patterns you notice!"]},{"cell_type":"markdown","metadata":{"id":"iEUR6G-cM_Qk"},"source":["#Optional: GPT-2 Transformer Model\n","\n","Please run the following cell (Getting GPT-2 Up and Running) as you discuss GPT-2, since it takes a while to execute.  \n","\n","**Note:** This section uses a different versions of some libraries than the rest of the notebook. If you're having issues, please either:\n","*   Copy this code over into a new notebook, OR\n","*   \"Factory reset runtime\" and then run this section. If you need to go back to the beginning of the notebook, reset again.\n"]},{"cell_type":"code","metadata":{"id":"vm1SoJXxMG9M","cellView":"form"},"source":["#@title Run: Getting GPT-2 Up and Running\n","\"\"\"\n","Install the GPT-2 fine-tuning library\n","\"\"\"\n","\n","!pip3 install -q gpt-2-simple\n","!pip3 install gast==0.2.2\n","!pip3 install -q tensorflow==1.15\n","\n","\"\"\"\n","Import libraries\n","\"\"\"\n","\n","import io\n","import os\n","import pickle\n","import zipfile\n","import requests\n","import tensorflow as tf\n","print(tf.__version__)\n","from zipfile import ZipFile\n","import gpt_2_simple as gpt2\n","from tqdm.notebook import tqdm\n","from bs4.element import Comment\n","from bs4 import BeautifulSoup as bs\n","\n","\"\"\"\n","Get the training data link\n","\"\"\"\n","\n","site = 'https://www.dropbox.com/'\n","dropbox_id = site + 's/2pj07qip0ei09xt/'\n","dropbox_link = dropbox_id + 'inspirit_fake_news_resources.zip?dl=1'\n","\n","\"\"\"\n","Extract the data from the DropBox link\n","\"\"\"\n","\n","r = requests.get(dropbox_link)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","\n","\"\"\"\n","Get the pickled data from the ZIP file\n","\"\"\"\n","\n","z.extractall()\n","basepath = '.'\n","path = os.path.join(basepath, 'train_val_data.pkl')\n","\n","\"\"\"\n","Load the pickle files with training and validation data\n","\"\"\"\n","\n","with open(path, 'rb') as f:\n","  train_data, val_data = pickle.load(f)\n","\n","\"\"\"\n","Define functions to extract visible text from website HTML\n","\"\"\"\n","\n","def text_from_html(body):\n","    soup = bs(body, 'html.parser')\n","    texts = soup.findAll(text=True)\n","    visible_texts = filter(tag_visible, texts)  \n","    return ' '.join((u\" \".join(t.strip() for t in visible_texts)).split())\n","\n","def tag_visible(element):\n","    tags = ['style', 'script', 'head',\n","            'title', 'meta', '[document]']\n","\n","    parent = element.parent.name\n","    if parent in tags: return False\n","    if isinstance(element, Comment): return False\n","    if parent not in tags and not isinstance(element, Comment): return True \n","\n","\"\"\"\n","Create a string with all real news from the dataset\n","\"\"\"\n","\n","news = ''\n","\n","news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(train_data) if data_point[2]==0)\n","news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(val_data) if data_point[2]==0)\n","\n","# for data_point in tqdm(train_data):\n","#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' '\n","\n","# for data_point in tqdm(val_data):\n","#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' ' \n","\n","\"\"\"\n","Load the GPT-2 model with pre-trained weights\n","\"\"\"\n","\n","model_name = \"124M\"\n","print(f\"Downloading {model_name} model...\")\n","gpt2.download_gpt2(model_name = model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IucOs2lPjBcR"},"source":["<center><img src=\"https://imgur.com/p16AuJH.jpg\" width=\"1000px\"></center>\n"]},{"cell_type":"markdown","metadata":{"id":"Jri_GRGtpAM-"},"source":["In the remaining code blocks, we build a fake news generation model based on GPT-2 (a transformer model). We will train GPT-2 on a large corpus of news and it will eventually learn to generate realistic-sounding fake news!  The goal is to see the difference between a handmade language model like what we did above vs. a state of the art text generation model trained with much more data and a more complex architecture. Most of the code is given because it is very specific to GPT-2, but please read through it and ask questions!"]},{"cell_type":"markdown","metadata":{"id":"siLZHpcIQcJv"},"source":["Above, we used two types of RNNs: Simple RNNs and LSTMs.  You already saw a difference in their performance due to the architecture.  Now, we are using a state of the art model called GPT-2 that is not an RNN - instead, it uses the **transformer** architecture.  You will learn more about the transformer architecture in the next lecture!\n","\n","You can check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/)."]},{"cell_type":"markdown","metadata":{"id":"hzu4t5yArMtj"},"source":["### Fine-tune the GPT-2 model\n","\n","Next, we dump all the news into a *.txt* file and fine-tune *GPT-2* on this text. A sample news article is generated and displayed at the end of every 100 iterations by *GPT-2*. Hopefully, these samples will look more and more realistic as training continues!"]},{"cell_type":"code","metadata":{"id":"wZS0cXUxb7vG"},"source":["#Dump the text into a .txt file and fine-tune the model\n","\n","news = news[:-1]\n","file_name = 'news.txt'\n","with open(file_name, 'w') as f: f.write(news)\n","\n","sess = gpt2.start_tf_sess()\n","gpt2.finetune(sess, file_name,\n","              model_name=model_name, steps=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ag1U3Bvraz-"},"source":["### Exercise 7: Test the model\n","\n","Now, we test the model by generating 10 sample fake news article. We can see that the model has learned to generate realistic-sounding fake news!\n","\n","use `gpt2.generate(sess)` to generate an example, and use a for loop to do more!"]},{"cell_type":"code","metadata":{"id":"houImEKYL4gO"},"source":["### Your code here ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXnAplM3NU2I"},"source":["##Discussion 6\n","\n","How did GPT2 do in comparison with our handmade language model?  Why do you think so?"]},{"cell_type":"markdown","metadata":{"id":"fqCWRiYpOSyD"},"source":["##Discussion 7\n","Check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/). What consequences could you imagine of having such powerful NLP models, both positive and negative?"]}]}